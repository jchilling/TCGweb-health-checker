import csv
import os
import sys
import asyncio
import argparse
import time
import gc  # ç”¨æ–¼å¼·åˆ¶åƒåœ¾å›æ”¶
# from dotenv import load_dotenv
from playwright.async_api import async_playwright

from crawler.web_crawler import WebCrawlerAgent
from reporter.report_generation import ReportGenerationAgent
from utils.extract_problematic_links import extract_error_links_from_json


def load_websites(path: str):
    websites_config = []
    with open(path, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            websites_config.append(row)
    return websites_config


async def process_single_website(semaphore: asyncio.Semaphore, browser, url: str, name: str, reporter: ReportGenerationAgent, depth: int, save_html: bool = True, enable_pagination: bool = True) -> dict:
    """è™•ç†å–®ä¸€ç¶²ç«™çš„ç•°æ­¥å‡½æ•¸ï¼Œä½¿ç”¨ semaphore æ§åˆ¶ä¸¦è¡Œæ•¸é‡"""
    
    async with semaphore:
        print(f"\nğŸ” é–‹å§‹è™•ç†ç¶²ç«™: {name or url}")
        
        # ç‚ºæ¯å€‹ç¶²ç«™å‰µå»ºç¨ç«‹çš„ crawler å¯¦ä¾‹ï¼Œå‚³å…¥æ˜¯å¦å„²å­˜HTMLçš„åƒæ•¸å’Œåˆ†é æ§åˆ¶åƒæ•¸
        crawler = WebCrawlerAgent(save_html_files=save_html, enable_pagination=enable_pagination)
        
        try:
            # è¨˜éŒ„é–‹å§‹æ™‚é–“
            start_time = time.time()
            
            # ä½¿ç”¨æ·±åº¦çˆ¬èŸ²
            crawl_results = await crawler.crawl_site(browser, url, name=name, max_depth=depth)
            
            # è¨ˆç®—çˆ¬å–è€—æ™‚
            crawl_duration = time.time() - start_time
            crawl_duration_formatted = f"{int(crawl_duration // 60)}åˆ†{int(crawl_duration % 60)}ç§’"
            
            # å„²å­˜é é¢æ‘˜è¦ç‚º JSON
            json_path = crawler.save_page_summary_to_json()
            if json_path:
                print(f"âœ… å·²å„²å­˜ {name or url} é é¢æ‘˜è¦åˆ°: {json_path}")
                
                # ç«‹å³æå–éŒ¯èª¤é€£çµä¸¦ç”¢ç”Ÿ CSV æª”æ¡ˆ
                extract_error_links_from_json(json_path)
            
            # å„²å­˜çˆ¬èŸ² log
            crawl_log_path = crawler.save_crawl_log()
            if crawl_log_path:
                print(f"ğŸ“ å·²å„²å­˜ {name or url} çˆ¬èŸ² log åˆ°: {crawl_log_path}")
            
            site_stats = {
                'site_name': name or url,
                'site_url': url,
                'crawl_results': crawl_results,  # å…§éƒ¨é é¢ status ç¢¼åˆ—è¡¨
                'page_summary': crawler.get_page_summary(),      
                'external_link_results': crawler.get_external_link_results(), 
                'crawl_duration': crawl_duration_formatted
            }
            
            # ç«‹å³å°‡é€™å€‹ç¶²ç«™çš„è³‡æ–™å¯«å…¥ Excel
            await reporter.add_site_to_excel(site_stats)
            
            print(f"âœ… ç¶²ç«™ '{name or url}' è™•ç†å®Œæˆï¼Œå…±çˆ¬å– {len(crawl_results)} å€‹é é¢ï¼Œè€—æ™‚ {crawl_duration_formatted}")
            
            return True
            
        except Exception as e:
            print(f"âŒ è™•ç†ç¶²ç«™ '{name or url}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
        finally:
            # é—œé–‰ crawler httpx client
            await crawler.close()
            # ç«‹å³æ‰‹å‹•æ¸…é™¤ crawler å…§éƒ¨çš„å¤§å‹å­—å…¸
            crawler.clear_memory()
            # å¼·åˆ¶ Python åŸ·è¡Œåƒåœ¾å›æ”¶
            gc.collect()


async def auto_shutdown_vm():
    """
    è‡ªå‹•é—œé–‰ GCE VM åŸ·è¡Œå€‹é«”
    """
    try:
        import subprocess
        
        # ç›´æ¥ä½¿ç”¨å›ºå®šçš„ VM åç¨±å’Œå€åŸŸ
        vm_name = "crawler-webcheck"
        zone = "asia-east1-c"
        
        print(f"ğŸ‰ ä»»å‹™å…¨éƒ¨å®Œæˆï¼Œæº–å‚™è‡ªå‹•é—œé–‰ VM: {vm_name}")
        print(f"ğŸ“ VM ä½ç½®: {zone}")
        
        # åŸ·è¡Œé—œæ©ŸæŒ‡ä»¤
        shutdown_cmd = f"gcloud compute instances stop {vm_name} --zone={zone} --quiet"
        print(f"ğŸ’» åŸ·è¡Œé—œæ©ŸæŒ‡ä»¤: {shutdown_cmd}")
        
        shutdown_result = subprocess.run(
            shutdown_cmd.split(),
            capture_output=True, text=True, timeout=60
        )
        
        if shutdown_result.returncode == 0:
            print("âœ… VM é—œæ©ŸæŒ‡ä»¤åŸ·è¡ŒæˆåŠŸ")
        else:
            print(f"âŒ VM é—œæ©ŸæŒ‡ä»¤åŸ·è¡Œå¤±æ•—: {shutdown_result.stderr}")
            
    except Exception as e:
        print(f"âš ï¸ è‡ªå‹•é—œæ©Ÿå¤±æ•—: {e}")
        print("â„¹ï¸ VM å°‡ä¿æŒé–‹å•Ÿç‹€æ…‹")            


async def main():
    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    parser = argparse.ArgumentParser(description='ç¶²ç«™çˆ¬èŸ²å’Œåˆ†æå·¥å…·')
    parser.add_argument('--depth', type=int, default=2, 
                       help='çˆ¬èŸ²çš„æœ€å¤§æ·±åº¦ (é è¨­: 2)')
    parser.add_argument('--config', type=str, default="config/websites.csv",
                       help='ç¶²ç«™è¨­å®šæª”æ¡ˆè·¯å¾‘ (é è¨­: config/websites.csv)')
    parser.add_argument('--concurrent', type=int, default=2,
                       help='åŒæ™‚è™•ç†çš„ç¶²ç«™æ•¸é‡ (é è¨­: 2)')
    parser.add_argument('--no-save-html', action='store_true',
                       help='ä¸å„²å­˜HTMLæª”æ¡ˆï¼Œåƒ…ç”¢ç”Ÿçµ±è¨ˆå ±å‘Š (æå‡æ•ˆèƒ½ï¼Œç¯€çœç£ç¢Ÿç©ºé–“)')
    parser.add_argument('--no-pagination', action='store_true',
                       help='ç¦ç”¨åˆ†é çˆ¬å–ï¼Œå°‡æœ‰åˆ†é åƒæ•¸çš„é é¢è¦–ç‚ºé‡è¤‡é é¢è·³é (æå‡æ•ˆèƒ½)')
    
    args = parser.parse_args()
    
    # å–å¾—å…¨åŸŸé è¨­å€¼
    global_depth = args.depth
    global_save_html = not args.no_save_html
    global_enable_pagination = not args.no_pagination
    
    #load_dotenv()
    
    if not os.path.exists(args.config):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¨­å®šæª”æ¡ˆ {args.config}")
        sys.exit(1)
    
    websites = load_websites(args.config)
    print(f"è¼‰å…¥äº† {len(websites)} å€‹ç¶²ç«™ï¼Œæœ€å¤§çˆ¬èŸ²æ·±åº¦: {global_depth}ï¼Œä¸¦è¡Œæ•¸é‡: {args.concurrent}")
    if global_save_html:
        print("ğŸ’¾ HTMLæª”æ¡ˆå„²å­˜: å•Ÿç”¨")
    else:
        print("ğŸš€ HTMLæª”æ¡ˆå„²å­˜: åœç”¨ (åƒ…ç”¢ç”Ÿçµ±è¨ˆå ±å‘Šï¼Œæå‡æ•ˆèƒ½)")
    
    if global_enable_pagination:
        print("ğŸ“„ åˆ†é çˆ¬å–: å•Ÿç”¨")
    else:
        print("âš¡ åˆ†é çˆ¬å–: åœç”¨ (åˆ†é è¦–ç‚ºé‡è¤‡é é¢è·³éï¼Œæå‡æ•ˆèƒ½)")

    # å‰µå»º semaphore ä¾†æ§åˆ¶ä¸¦è¡Œæ•¸é‡
    semaphore = asyncio.Semaphore(args.concurrent)
    
    # åˆå§‹åŒ–å ±å‘Šç”Ÿæˆå™¨
    reporter = ReportGenerationAgent()
    output_path = reporter.initialize_excel_report()
    print(f"Excel å ±å‘Šæª”æ¡ˆåˆå§‹åŒ–å®Œæˆ: {output_path}")
    
    # å–å¾—å·²è™•ç†çš„ç¶²ç«™URLåˆ—è¡¨ï¼ˆç”¨æ–¼æ–·é»çºŒçˆ¬ï¼‰
    processed_urls = reporter.get_processed_urls()
    
    # éæ¿¾æ‰å·²è™•ç†çš„ç¶²ç«™
    websites_to_process = []
    
    for site in websites:
        url = site["URL"]
        if url.strip() in processed_urls:
            continue
            
        websites_to_process.append(site)
    
    print(f"ğŸ“‹ ç¸½å…± {len(websites)} å€‹ç¶²ç«™ï¼Œå‰©é¤˜ {len(websites_to_process)} å€‹å¾…è™•ç†")
    
    if not websites_to_process:
        print("ğŸ‰ æ‰€æœ‰ç¶²ç«™éƒ½å·²è™•ç†å®Œæˆï¼")
        reporter.finalize_excel_report()
        print(f"ğŸ“„ å ±å‘Šå·²å„²å­˜åˆ°: {output_path}")
        
        # --- è‡ªå‹•é—œæ©ŸåŠŸèƒ½ ---
        await auto_shutdown_vm()
        return

    async with async_playwright() as p:
        browser = await p.chromium.launch()
        
        crawl_success = True  # é è¨­ç‚ºæˆåŠŸ
        
        try:
            print(f"\nğŸš€ é–‹å§‹ä¸¦è¡Œè™•ç† {len(websites_to_process)} å€‹ç¶²ç«™...")
            start_time = time.time()
            
            # å‰µå»ºæ‰€æœ‰ç¶²ç«™çš„è™•ç†ä»»å‹™
            tasks = []
            for site in websites_to_process:
                # è®€å– CSV ä¸­çš„ç‰¹å®šè¨­å®šï¼Œå¦‚æœç‚ºç©ºæˆ–ç„¡æ•ˆï¼Œå‰‡ä½¿ç”¨å…¨åŸŸé è¨­å€¼
                
                try:
                    # å˜—è©¦è®€å– 'depth'ï¼Œå¦‚æœå¤±æ•—æˆ–ç‚ºç©ºï¼Œä½¿ç”¨ global_depth
                    site_depth = int(site.get('depth')) if site.get('depth') else global_depth
                except (ValueError, TypeError):
                    site_depth = global_depth
                    
                # å˜—è©¦è®€å– 'save_html'
                if site.get('save_html', '').lower() == 'true':
                    site_save_html = True
                elif site.get('save_html', '').lower() == 'false':
                    site_save_html = False
                else:
                    site_save_html = global_save_html # CSV ä¸­ç‚ºç©ºï¼Œä½¿ç”¨å…¨åŸŸè¨­å®š
                
                # å˜—è©¦è®€å– 'pagination'
                if site.get('pagination', '').lower() == 'true':
                    site_enable_pagination = True
                elif site.get('pagination', '').lower() == 'false':
                    site_enable_pagination = False
                else:
                    site_enable_pagination = global_enable_pagination # CSV ä¸­ç‚ºç©ºï¼Œä½¿ç”¨å…¨åŸŸè¨­å®š

                url = site["URL"]
                name = site.get("name", "")

                # å»ºç«‹ä»»å‹™æ™‚ï¼Œå‚³å…¥ç‰¹å®šæ–¼è©²ç¶²ç«™çš„åƒæ•¸
                task = process_single_website(
                    semaphore, 
                    browser, 
                    url, 
                    name, 
                    reporter, 
                    site_depth,            
                    site_save_html,        
                    site_enable_pagination 
                )
                tasks.append(task)
            
            # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰ä»»å‹™ï¼Œreturn_exceptions=True ç¢ºä¿å–®ä¸€å¤±æ•—ä¸æœƒå½±éŸ¿å…¶ä»–ä»»å‹™
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # çµ±è¨ˆçµæœ
            successful_sites = sum(1 for result in results if result is True)
            failed_sites = len(websites_to_process) - successful_sites
            total_duration = time.time() - start_time
            total_duration_formatted = f"{int(total_duration // 60)}åˆ†{int(total_duration % 60)}ç§’"
            
            print(f"\nğŸ‰ ä¸¦è¡Œè™•ç†å®Œæˆ!")
            print(f"ğŸ“Š æˆåŠŸè™•ç†: {successful_sites} å€‹ç¶²ç«™")
            print(f"âŒ å¤±æ•—: {failed_sites} å€‹ç¶²ç«™") 
            print(f"â±ï¸ ç¸½è€—æ™‚: {total_duration_formatted}")
                        
        except Exception as e:
            print(f"\nğŸ’¥ çˆ¬èŸ²åŸ·è¡Œéç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            crawl_success = False  # æœ‰ç•°å¸¸å°±ä¸é—œæ©Ÿ
            
        finally:
            await browser.close()
            
            # å®Œæˆ Excel å ±å‘Š
            reporter.finalize_excel_report()
            print(f"\nğŸ“„ å ±å‘Šå·²å„²å­˜åˆ°: {output_path}")
            print(f"âœ… ç¸½å…±è™•ç†äº† {len(websites_to_process)} å€‹æ–°ç¶²ç«™")
            
            # åªæœ‰åœ¨æˆåŠŸæ™‚æ‰é—œæ©Ÿ
            if crawl_success:
                await auto_shutdown_vm()
            else:
                print("ğŸ”§ ç”±æ–¼åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼ŒVM å°‡ä¿æŒé–‹å•Ÿç‹€æ…‹ä»¥ä¾¿é™¤éŒ¯")

if __name__ == "__main__":
    asyncio.run(main())
