import csv
import os
import sys
import asyncio
import argparse
import time
import multiprocessing
from datetime import datetime, timedelta
from playwright.async_api import async_playwright
import zipfile
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

from crawler.web_crawler import WebCrawlerAgent
from reporter.report_generation_mp import ReportGenerationAgent
from utils.extract_problematic_links import extract_error_links_from_json


def load_websites(path: str):
    """è¼‰å…¥ç¶²ç«™è¨­å®šæª”"""
    websites_config = []
    with open(path, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            websites_config.append(row)
    return websites_config


async def _async_crawl_worker(site_config: dict) -> dict:
    """
    Subprocess ä¸­ asyncio è¿´åœˆå…§åŸ·è¡Œçš„çœŸæ­£çˆ¬èŸ²
    """
    url = site_config["URL"]
    name = site_config.get("name", "")
    depth = site_config["global_depth"]
    save_html = site_config["global_save_html"]
    enable_pagination = site_config["global_enable_pagination"]

    print(f"\nğŸ” [PID {os.getpid()}] é–‹å§‹è™•ç†ç¶²ç«™: {name or url}")
    
    try:
        # åœ¨ subprocess ä¸­å»ºç«‹ crawler å’Œ playwright
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            crawler = WebCrawlerAgent(save_html_files=save_html, enable_pagination=enable_pagination)
            
            start_time = time.time()
            
            # åŸ·è¡Œçˆ¬èŸ²
            crawl_results = await crawler.crawl_site(browser, url, name=name, max_depth=depth)
            crawl_duration = time.time() - start_time
            crawl_duration_formatted = f"{int(crawl_duration // 60)}åˆ†{int(crawl_duration % 60)}ç§’"
            
            page_summary = crawler.get_page_summary()
            external_link_results = crawler.get_external_link_results()

            # å„²å­˜ JSON/Log
            json_path = crawler.save_page_summary_to_json()
            if json_path:
                extract_error_links_from_json(json_path)
            crawler.save_crawl_log()

            # é å…ˆè¨ˆç®—çµ±è¨ˆæ•¸æ“šçµ¦ Excel
            one_year_ago = datetime.now() - timedelta(days=365)
            total_pages = len(crawl_results)
            failed_pages = sum(1 for status in crawl_results if status >= 400 or status == 0)
            failed_external_links = sum(1 for link_info in external_link_results.values() 
                                       if link_info.get('status', 0) >= 400 or link_info.get('status', 0) == 0)
            
            # è¨ˆç®—æ—¥æœŸç›¸é—œçµ±è¨ˆ
            today = datetime.now().date()
            pages_with_date = 0
            no_date_pages = 0
            outdated_pages = 0
            past_dates = []  # ä»Šå¤©æˆ–ä»¥å‰çš„æ—¥æœŸ
            future_dates = []  # æœªä¾†æ—¥æœŸ
            
            for url_key, page_info in page_summary.items():
                last_updated = page_info.get('last_updated', '')
                
                # çµ±è¨ˆç„¡æ—¥æœŸçš„é é¢
                if last_updated == "[ç„¡æ—¥æœŸ]" or last_updated == "[çˆ¬å–å¤±æ•—]" or not last_updated:
                    no_date_pages += 1
                    continue
                
                try:
                    update_date = datetime.strptime(last_updated, '%Y-%m-%d')
                    update_date_only = update_date.date()
                    
                    if update_date_only <= today:
                        past_dates.append(update_date)
                        # æª¢æŸ¥æ˜¯å¦ç‚ºä¸€å¹´å‰çš„å…§å®¹
                        if update_date < one_year_ago:
                            outdated_pages += 1
                    else:
                        future_dates.append(update_date)
                        
                except ValueError:
                    no_date_pages += 1
                    continue
            
            # è¨ˆç®—æœ€æ–°æ›´æ–°æ—¥æœŸï¼šå„ªå…ˆä½¿ç”¨éå»æ—¥æœŸçš„æœ€æ–°å€¼ï¼Œæ²’æœ‰æ‰ç”¨æœ€æ¥è¿‘ä»Šå¤©çš„æœªä¾†æ—¥æœŸ
            if past_dates:
                latest_update = max(past_dates).strftime('%Y-%m-%d')
            elif future_dates:
                latest_update = min(future_dates).strftime('%Y-%m-%d')
            else:
                latest_update = "ç„¡æœ‰æ•ˆæ—¥æœŸ"
            
            # è¨ˆç®—ä¸€å¹´å‰å…§å®¹çš„æ¯”ä¾‹
            pages_with_date = len(past_dates) + len(future_dates)
            outdated_percentage = (outdated_pages / pages_with_date * 100) if pages_with_date > 0 else 0
            
            # å»ºç«‹å°å‹çµæœå­—å…¸
            stats_for_excel = {
                'site_name': name or url,
                'site_url': url,
                'total_pages': total_pages,
                'pages_with_date': pages_with_date,
                'no_date_pages': no_date_pages,
                'latest_update': latest_update,
                'outdated_pages': outdated_pages,
                'outdated_percentage': round(outdated_percentage, 2),
                'failed_pages': failed_pages,
                'failed_external_links': failed_external_links,
                'total_external_links': len(external_link_results),
                'crawl_duration': crawl_duration_formatted
            }
            
            # æ¸…ç†ä¸¦é—œé–‰
            del page_summary
            del external_link_results
            await crawler.close()
            crawler.clear_memory()
            del crawler
            await browser.close()
            
            print(f"âœ… [PID {os.getpid()}] ç¶²ç«™ '{name or url}' è™•ç†å®Œæˆ")
            return stats_for_excel
                
    except Exception as e:
        print(f"âŒ [PID {os.getpid()}] è™•ç†ç¶²ç«™ '{name or url}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        try:
            if 'crawler' in locals() and crawler:
                await crawler.close()
                crawler.clear_memory()
                del crawler
            if 'browser' in locals() and browser:
                await browser.close()
            
            # çµ¦äºˆ 5 ç§’ç·©è¡æ™‚é–“
            await asyncio.sleep(5)
            
        except Exception as cleanup_e:
            print(f"ğŸ’¥ [PID {os.getpid()}] åœ¨éŒ¯èª¤æ¸…ç†ä¸­ç™¼ç”Ÿäº†é¡å¤–éŒ¯èª¤: {cleanup_e}")
            
        return None  # ç™¼ç”ŸéŒ¯èª¤æ™‚è¿”å› None


def run_crawl_task(site_config: dict) -> dict:
    """
    multiprocessing.Pool å‘¼å«çš„åŒ…è£å‡½æ•¸
    å®ƒæœƒå»ºç«‹è‡ªå·±çš„ asyncio è¿´åœˆ
    """    
    try:
        return asyncio.run(_async_crawl_worker(site_config))
    except Exception as e:
        print(f"ğŸ’¥ [PID {os.getpid()}] åŸ·è¡Œä»»å‹™ '{site_config.get('name', 'N/A')}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None 


def pack_and_send_email(excel_report_path):
    """
    æ‰“åŒ… (Excel + Log + Assets) ä¸¦ç™¼é€ Email
    """
    GMAIL_USER = os.getenv("GMAIL_USER")
    GMAIL_APP_PASSWORD = os.getenv("GMAIL_APP_PASSWORD")
    TO_EMAIL = os.getenv("TO_EMAIL", GMAIL_USER)  # å¦‚æœæ²’è¨­æ”¶ä»¶äººï¼Œé è¨­å¯„çµ¦è‡ªå·±

    if not GMAIL_USER or not GMAIL_APP_PASSWORD:
        print("âŒ éŒ¯èª¤ï¼šæœªè¨­å®š GMAIL_USER æˆ– GMAIL_APP_PASSWORD ç’°å¢ƒè®Šæ•¸")
        print("ğŸ’¡ è«‹ç¢ºèª .env æª”æ¡ˆå·²å»ºç«‹ä¸”åŒ…å«å¿…è¦çš„è¨­å®š")
        return
    
    print(f"ğŸ“§ ä½¿ç”¨ Gmail å¸³è™Ÿ: {GMAIL_USER}")
    print(f"ğŸ“¬ æ”¶ä»¶äºº: {TO_EMAIL}")
    
    # zip æª”å
    timestamp = datetime.now().strftime('%Y%m')
    zip_filename = f"website_check_results_{timestamp}.zip"
    
    try:
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            
            # Excel
            if os.path.exists(excel_report_path):
                print(f"åŠ å…¥å ±å‘Š: {excel_report_path}")
                zipf.write(excel_report_path, os.path.basename(excel_report_path))
            else:
                print(f"âš ï¸ æ‰¾ä¸åˆ°å ±å‘Šæª”: {excel_report_path}")
            
            # crawler_execution.log
            vm_log_path = os.path.expanduser('~/crawler_execution.log')
            if os.path.exists(vm_log_path):
                print(f"åŠ å…¥æ—¥èªŒ: {vm_log_path}")
                zipf.write(vm_log_path, "crawler_execution.log")
            else:
                print(f"âš ï¸ æ‰¾ä¸åˆ°æ—¥èªŒæª”: {vm_log_path}")

            # assets
            if os.path.exists("assets"):
                for root, dirs, files in os.walk("assets"):
                    for file in files:
                        # è·³é html æª”
                        # if file.lower().endswith('.html'):
                            # continue
                        
                        file_path = os.path.join(root, file)
                        # åœ¨ zip ä¸­çš„è·¯å¾‘ (ä¿æŒè³‡æ–™å¤¾çµæ§‹)
                        arcname = os.path.relpath(file_path, start=".")
                        zipf.write(file_path, arcname)
                print(f"åŠ å…¥å„ç¶²ç«™è©³ç´°æª”æ¡ˆ")
            else:
                print(f"âš ï¸ æ‰¾ä¸åˆ° assets è³‡æ–™å¤¾")
            
        print(f"âœ… å£“ç¸®å®Œæˆ: {zip_filename}\n")

        # æª¢æŸ¥ zip æª”æ¡ˆå¤§å°
        zip_size_mb = os.path.getsize(zip_filename) / 1024 / 1024
        
        if zip_size_mb > 25:
            print(f"âš ï¸ è­¦å‘Šï¼š{zip_size_mb:.2f} è¶…é Gmail 25MB é™åˆ¶ï¼Œå¯èƒ½ç„¡æ³•ç™¼é€")

        # é€ Email
        print("ğŸ“§ æ­£åœ¨ç™¼é€ Email...")
        msg = MIMEMultipart()
        msg['Subject'] = f"ç¶²ç«™æª¢æ ¸çˆ¬èŸ²å®Œæ•´è³‡æ–™çµæœ - {datetime.now().strftime('%Y/%m')}"
        msg['From'] = GMAIL_USER
        msg['To'] = TO_EMAIL
        
        body = (
            "çˆ¬èŸ²ä»»å‹™å·²å®Œæˆã€‚\n\n"
            "é™„ä»¶æ‡‰åŒ…å«ï¼š\n"
            "1. Excel çµ±è¨ˆå ±å‘Š\n"
            "2. åŸ·è¡Œæ—¥èªŒ (crawler_execution.log)\n"
            "3. Assets è³‡æ–™å¤¾å…§å®¹\n\n"
            f"å£“ç¸®æª”å¤§å°: {zip_size_mb:.2f} MB\n\n"
            "(ç”± GCP VM è‡ªå‹•ç™¼é€)"
        )
        msg.attach(MIMEText(body, 'plain'))
        
        # Zip é™„ä»¶
        with open(zip_filename, "rb") as f:
            part = MIMEApplication(f.read(), Name=zip_filename)
        
        part['Content-Disposition'] = f'attachment; filename="{zip_filename}"'
        msg.attach(part)
        
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)
            server.send_message(msg)
            print("âœ… Email ç™¼é€æˆåŠŸï¼\n")
            
        # ç™¼é€å¾Œåˆªé™¤ zip æª”ä»¥ç¯€çœç©ºé–“
        os.remove(zip_filename)

    except Exception as e:
        print(f"âŒ æ‰“åŒ…æˆ–ç™¼é€ Email å¤±æ•—: {e}\n")
        # import traceback
        # print(f"è©³ç´°éŒ¯èª¤è³‡è¨Š: {traceback.format_exc()}")


def auto_shutdown_vm():
    """
    è‡ªå‹•é—œé–‰ GCE VM åŸ·è¡Œå€‹é«”
    """
    try:
        import subprocess
        
        # ç›´æ¥ä½¿ç”¨å›ºå®šçš„ VM åç¨±å’Œå€åŸŸ
        vm_name = "crawler-webcheck-mpfast"
        zone = "asia-east1-c"
        
        print(f"ğŸ‰ ä»»å‹™å…¨éƒ¨å®Œæˆï¼Œæº–å‚™è‡ªå‹•é—œé–‰ VM: {vm_name}")
        print(f"ğŸ“ VM ä½ç½®: {zone}")
        
        # åŸ·è¡Œé—œæ©ŸæŒ‡ä»¤
        shutdown_cmd = f"gcloud compute instances stop {vm_name} --zone={zone} --quiet"
        print(f"ğŸ’» åŸ·è¡Œé—œæ©ŸæŒ‡ä»¤: {shutdown_cmd}")
        
        shutdown_result = subprocess.run(
            shutdown_cmd.split(),
            capture_output=True, text=True, timeout=60
        )
        
        if shutdown_result.returncode == 0:
            print("âœ… VM é—œæ©ŸæŒ‡ä»¤åŸ·è¡ŒæˆåŠŸ")
        else:
            print(f"âŒ VM é—œæ©ŸæŒ‡ä»¤åŸ·è¡Œå¤±æ•—: {shutdown_result.stderr}")
            
    except Exception as e:
        print(f"âš ï¸ è‡ªå‹•é—œæ©Ÿå¤±æ•—: {e}")
        print("â„¹ï¸ VM å°‡ä¿æŒé–‹å•Ÿç‹€æ…‹")


def main():
    """
    ä¸»å‡½æ•¸
    """
    parser = argparse.ArgumentParser(description='ç¶²ç«™çˆ¬èŸ²å’Œåˆ†æå·¥å…· (Multiprocessing ç‰ˆæœ¬)')
    parser.add_argument('--depth', type=int, default=2, 
                       help='çˆ¬èŸ²çš„æœ€å¤§æ·±åº¦ (é è¨­: 2)')
    parser.add_argument('--config', type=str, default="config/websites.csv",
                       help='ç¶²ç«™è¨­å®šæª”æ¡ˆè·¯å¾‘ (é è¨­: config/websites.csv)')
    parser.add_argument('--concurrent', type=int, default=2,
                       help='åŒæ™‚è™•ç†çš„ç¶²ç«™æ•¸é‡ (é è¨­: 2)')
    parser.add_argument('--no-save-html', action='store_true',
                       help='ä¸å„²å­˜HTMLæª”æ¡ˆï¼Œåƒ…ç”¢ç”Ÿçµ±è¨ˆå ±å‘Š (æå‡æ•ˆèƒ½ï¼Œç¯€çœç£ç¢Ÿç©ºé–“)')
    parser.add_argument('--no-pagination', action='store_true',
                       help='ç¦ç”¨åˆ†é çˆ¬å–ï¼Œå°‡æœ‰åˆ†é åƒæ•¸çš„é é¢è¦–ç‚ºé‡è¤‡é é¢è·³é (æå‡æ•ˆèƒ½)')
    parser.add_argument('--max-mem-mb', type=int, default=1024,
                       help='subprocess è¨˜æ†¶é«”ä¸Šé™ (MB)ï¼Œè¶…éæ­¤å€¼å°‡è‡ªå‹•å›æ”¶ (é è¨­: 1024)')
    
    args = parser.parse_args()
    
    # å–å¾—å…¨åŸŸé è¨­å€¼
    global_depth = args.depth
    global_save_html = not args.no_save_html
    global_enable_pagination = not args.no_pagination
    
    if not os.path.exists(args.config):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¨­å®šæª”æ¡ˆ {args.config}")
        sys.exit(1)
    
    websites = load_websites(args.config)
    print(f"è¼‰å…¥äº† {len(websites)} å€‹ç¶²ç«™ï¼Œæœ€å¤§çˆ¬èŸ²æ·±åº¦: {global_depth}ï¼Œä¸¦è¡Œæ•¸é‡: {args.concurrent}")
    if global_save_html:
        print("ğŸ’¾ HTMLæª”æ¡ˆå„²å­˜: å•Ÿç”¨")
    else:
        print("ğŸš€ HTMLæª”æ¡ˆå„²å­˜: åœç”¨ (åƒ…ç”¢ç”Ÿçµ±è¨ˆå ±å‘Šï¼Œæå‡æ•ˆèƒ½)")
    
    if global_enable_pagination:
        print("ğŸ“„ åˆ†é çˆ¬å–: å•Ÿç”¨")
    else:
        print("âš¡ åˆ†é çˆ¬å–: åœç”¨ (åˆ†é è¦–ç‚ºé‡è¤‡é é¢è·³éï¼Œæå‡æ•ˆèƒ½)")

    print("ğŸš€ å•Ÿå‹• Multiprocessing ç¶²ç«™çˆ¬èŸ²...")

    # åˆå§‹åŒ– Reporter
    reporter = ReportGenerationAgent()
    output_path = reporter.initialize_excel_report()
    print(f"Excel å ±å‘Šæª”æ¡ˆåˆå§‹åŒ–å®Œæˆ: {output_path}")
    
    processed_urls = reporter.get_processed_urls()
    
    # ä»»å‹™åˆ—è¡¨
    websites_to_process = []
    for site in websites:
        url = site["URL"]
        if url.strip() in processed_urls:
            continue
            
        # åƒæ•¸åˆä½µè™•ç†  
        try:
            # è®€å–è¡¨æ ¼å…§çš„ depth å€¼
            csv_depth = int(site.get('depth')) if site.get('depth') else None
            # åªæœ‰ç•¶å…¨åŸŸæ·±åº¦å¤§æ–¼è¡¨æ ¼å…§æ·±åº¦æ™‚ï¼Œæ‰ä½¿ç”¨è¡¨æ ¼å…§çš„è¼ƒå°å€¼
            if csv_depth is not None and global_depth > csv_depth:
                site_depth = csv_depth
            else:
                site_depth = global_depth
        except (ValueError, TypeError):
            site_depth = global_depth
        site['global_depth'] = site_depth
        
        # å˜—è©¦è®€å– 'save_html'
        if site.get('save_html', '').lower() == 'true':
            site['global_save_html'] = True
        elif site.get('save_html', '').lower() == 'false':
            site['global_save_html'] = False
        else:
            site['global_save_html'] = global_save_html  # CSV ä¸­ç‚ºç©ºï¼Œä½¿ç”¨å…¨åŸŸè¨­å®š
        
        # å˜—è©¦è®€å– 'pagination'
        if site.get('pagination', '').lower() == 'true':
            site['global_enable_pagination'] = True
        elif site.get('pagination', '').lower() == 'false':
            site['global_enable_pagination'] = False
        else:
            site['global_enable_pagination'] = global_enable_pagination  # CSV ä¸­ç‚ºç©ºï¼Œä½¿ç”¨å…¨åŸŸè¨­å®š
        
        site['global_max_mem_mb'] = args.max_mem_mb
            
        websites_to_process.append(site)
    
    print(f"ğŸ“‹ ç¸½å…± {len(websites)} å€‹ç¶²ç«™ï¼Œå‰©é¤˜ {len(websites_to_process)} å€‹å¾…è™•ç†")
    
    if not websites_to_process:
        print("ğŸ‰ æ‰€æœ‰ç¶²ç«™éƒ½å·²è™•ç†å®Œæˆï¼")
        reporter.finalize_excel_report()
        print(f"ğŸ“„ å ±å‘Šå·²å„²å­˜åˆ°: {output_path}")
        
        print("æº–å‚™æ‰“åŒ…ä¸¦ç™¼é€å ±å‘Š...")
        pack_and_send_email(output_path)
        
        # é—œæ©Ÿ
        auto_shutdown_vm()
        return

    # ä½¿ç”¨ multiprocessing.Pool
    print(f"\nğŸš€ å•Ÿå‹• {args.concurrent} å€‹ä¸¦è¡Œè™•ç†ç¨‹åºï¼Œæ¯å€‹ä»»å‹™å¾Œé‡å•Ÿ (maxtasksperchild=1)")
    start_time = time.time()
    
    crawl_success = True
    successful_sites = 0
    failed_sites = 0
    
    try:
        with multiprocessing.Pool(processes=args.concurrent, maxtasksperchild=1) as pool:

            # ä½¿ç”¨ imap_unordered ä¾†å³æ™‚å–å¾— worker çµæœ
            results = pool.imap_unordered(run_crawl_task, websites_to_process)
            
            # Main process æ¥æ”¶å¾ sub process å‚³å›çš„çµæœ
            for stats_for_excel in results:
                if stats_for_excel:
                    # å‘¼å« add_site_to_excel å¯«å…¥çµæœ
                    try:
                        crawl_date = datetime.now().strftime('%Y-%m-%d %H:%M')
                        stats_for_excel['crawl_date'] = crawl_date
                        
                        reporter.add_site_to_excel(stats_for_excel)
                        successful_sites += 1
                    except Exception as e:
                        print(f"âŒ å¯«å…¥ Excel å¤±æ•—: {e}")
                        failed_sites += 1
                else:
                    failed_sites += 1

        total_duration = time.time() - start_time
        total_duration_formatted = f"{int(total_duration // 60)}åˆ†{int(total_duration % 60)}ç§’"
        
        print(f"\nğŸ‰ ä¸¦è¡Œè™•ç†å®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸè™•ç†: {successful_sites} å€‹ç¶²ç«™")
        print(f"âŒ å¤±æ•—: {failed_sites} å€‹ç¶²ç«™") 
        print(f"â±ï¸ ç¸½è€—æ™‚: {total_duration_formatted}")

    except Exception as e:
        print(f"\nğŸ’¥ è™•ç†éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        crawl_success = False

    finally:
        # Main process å®Œæˆå ±å‘Š
        reporter.finalize_excel_report()
        print(f"\nğŸ“„ å ±å‘Šå·²å„²å­˜åˆ°: {output_path}")
        print(f"âœ… ç¸½å…±è™•ç†äº† {len(websites_to_process)} å€‹æ–°ç¶²ç«™")
        
        if crawl_success:
            print("ğŸ‰ ä»»å‹™å…¨éƒ¨å®Œæˆï¼Œæº–å‚™æ‰“åŒ…ä¸¦ç™¼é€å ±å‘Š...")
            pack_and_send_email(output_path)
            print("æº–å‚™é—œæ©Ÿ...")
            auto_shutdown_vm()
        else:
            print("ğŸ”§ ç”±æ–¼åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼ŒVM å°‡ä¿æŒé–‹å•Ÿç‹€æ…‹ä»¥ä¾¿é™¤éŒ¯")


if __name__ == "__main__":
    # ç¢ºä¿ multiprocessing åœ¨ macOS/Windows ä¸Šæ­£å¸¸é‹ä½œ
    multiprocessing.freeze_support() 
    main()