import csv
import os
import sys
import asyncio
import argparse
import time
import multiprocessing
import psutil
from datetime import datetime, timedelta
from playwright.async_api import async_playwright

from crawler.web_crawler import WebCrawlerAgent
from reporter.report_generation_mp import ReportGenerationAgent
from utils.extract_problematic_links import extract_error_links_from_json


def load_websites(path: str):
    """è¼‰å…¥ç¶²ç«™è¨­å®šæª”"""
    websites_config = []
    with open(path, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            websites_config.append(row)
    return websites_config


async def _async_crawl_worker(site_config: dict) -> dict:
    """
    Subprocess ä¸­ asyncio è¿´åœˆå…§åŸ·è¡Œçš„çœŸæ­£çˆ¬èŸ²
    """
    url = site_config["URL"]
    name = site_config.get("name", "")
    depth = site_config["global_depth"]
    save_html = site_config["global_save_html"]
    enable_pagination = site_config["global_enable_pagination"]

    print(f"\nğŸ” [PID {os.getpid()}] é–‹å§‹è™•ç†ç¶²ç«™: {name or url}")
    
    try:
        # åœ¨å­é€²ç¨‹ä¸­å»ºç«‹ crawler å’Œ playwright  
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            crawler = WebCrawlerAgent(save_html_files=save_html, enable_pagination=enable_pagination)
            
            start_time = time.time()
            
            # 1. åŸ·è¡Œçˆ¬èŸ²
            crawl_results = await crawler.crawl_site(browser, url, name=name, max_depth=depth)
            crawl_duration = time.time() - start_time
            crawl_duration_formatted = f"{int(crawl_duration // 60)}åˆ†{int(crawl_duration % 60)}ç§’"
            
            # 2. æå–å¤§å‹çµæœè³‡æ–™
            page_summary = crawler.get_page_summary()
            external_link_results = crawler.get_external_link_results()

            # 3. å„²å­˜ JSON/Log
            json_path = crawler.save_page_summary_to_json()
            if json_path:
                extract_error_links_from_json(json_path)
            crawler.save_crawl_log()

            # 4. é å…ˆè¨ˆç®—çµ±è¨ˆæ•¸æ“šçµ¦ Excel
            one_year_ago = datetime.now() - timedelta(days=365)
            total_pages = len(crawl_results)
            failed_pages = sum(1 for status in crawl_results if status >= 400 or status == 0)
            failed_external_links = sum(1 for link_info in external_link_results.values() 
                                       if link_info.get('status', 0) >= 400 or link_info.get('status', 0) == 0)
            
            # è¨ˆç®—æ—¥æœŸç›¸é—œçµ±è¨ˆ
            today = datetime.now().date()
            pages_with_date = 0
            no_date_pages = 0
            outdated_pages = 0
            past_dates = []  # ä»Šå¤©æˆ–ä»¥å‰çš„æ—¥æœŸ
            future_dates = []  # æœªä¾†æ—¥æœŸ
            
            for url_key, page_info in page_summary.items():
                last_updated = page_info.get('last_updated', '')
                
                # çµ±è¨ˆç„¡æ—¥æœŸçš„é é¢
                if last_updated == "[ç„¡æ—¥æœŸ]" or last_updated == "[çˆ¬å–å¤±æ•—]" or not last_updated:
                    no_date_pages += 1
                    continue
                
                try:
                    update_date = datetime.strptime(last_updated, '%Y-%m-%d')
                    update_date_only = update_date.date()
                    
                    if update_date_only <= today:
                        past_dates.append(update_date)
                        # æª¢æŸ¥æ˜¯å¦ç‚ºä¸€å¹´å‰çš„å…§å®¹
                        if update_date < one_year_ago:
                            outdated_pages += 1
                    else:
                        future_dates.append(update_date)
                        
                except ValueError:
                    no_date_pages += 1
                    continue
            
            # è¨ˆç®—æœ€æ–°æ›´æ–°æ—¥æœŸï¼šå„ªå…ˆä½¿ç”¨éå»æ—¥æœŸçš„æœ€æ–°å€¼ï¼Œæ²’æœ‰æ‰ç”¨æœ€æ¥è¿‘ä»Šå¤©çš„æœªä¾†æ—¥æœŸ
            if past_dates:
                latest_update = max(past_dates).strftime('%Y-%m-%d')
            elif future_dates:
                latest_update = min(future_dates).strftime('%Y-%m-%d')
            else:
                latest_update = "ç„¡æœ‰æ•ˆæ—¥æœŸ"
            
            # è¨ˆç®—ä¸€å¹´å‰å…§å®¹çš„æ¯”ä¾‹
            pages_with_date = len(past_dates) + len(future_dates)
            outdated_percentage = (outdated_pages / pages_with_date * 100) if pages_with_date > 0 else 0
            
            # 5. å»ºç«‹å°å‹çµæœå­—å…¸
            stats_for_excel = {
                'site_name': name or url,
                'site_url': url,
                'total_pages': total_pages,
                'pages_with_date': pages_with_date,
                'no_date_pages': no_date_pages,
                'latest_update': latest_update,
                'outdated_pages': outdated_pages,
                'outdated_percentage': round(outdated_percentage, 2),
                'failed_pages': failed_pages,
                'failed_external_links': failed_external_links,
                'total_external_links': len(external_link_results),
                'crawl_duration': crawl_duration_formatted
            }
            
            # 6. æ¸…ç†ä¸¦é—œé–‰
            del page_summary
            del external_link_results
            await crawler.close()
            crawler.clear_memory()
            del crawler
            await browser.close()
            
            print(f"âœ… [PID {os.getpid()}] ç¶²ç«™ '{name or url}' è™•ç†å®Œæˆ")
            return stats_for_excel
                
    except Exception as e:
        print(f"âŒ [PID {os.getpid()}] è™•ç†ç¶²ç«™ '{name or url}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # ç™¼ç”ŸéŒ¯èª¤æ™‚ä¹Ÿè¦é€²è¡Œæ¸…ç†å’Œç­‰å¾…
        try:
            if 'crawler' in locals() and crawler:
                await crawler.close()
                crawler.clear_memory()
                del crawler
            if 'browser' in locals() and browser:
                await browser.close()
            
            # çµ¦äºˆ 5 ç§’ç·©è¡æ™‚é–“
            await asyncio.sleep(5)
            
        except Exception as cleanup_e:
            print(f"ğŸ’¥ [PID {os.getpid()}] åœ¨éŒ¯èª¤æ¸…ç†ä¸­ç™¼ç”Ÿäº†é¡å¤–éŒ¯èª¤: {cleanup_e}")
            
        return None  # ç™¼ç”ŸéŒ¯èª¤æ™‚è¿”å› None


def run_crawl_task(site_config: dict) -> dict:
    """
    multiprocessing.Pool å‘¼å«çš„åŒ…è£å‡½æ•¸
    å®ƒæœƒå»ºç«‹è‡ªå·±çš„ asyncio è¿´åœˆ
    ä¸¦ä¸”è‡ªæˆ‘ç›£æ§è¨˜æ†¶é«”ï¼Œè¶…æ¨™æ™‚è‡ªå‹•å›æ”¶
    """
    
    # è®€å–ä¸»é€²ç¨‹å‚³ä¾†çš„è¨˜æ†¶é«”ä¸Šé™
    max_mem_mb = site_config.get('global_max_mem_mb', 1024)  # é è¨­ 1024MB (1GB)
    
    # æª¢æŸ¥è‡ªå·±çš„è¨˜æ†¶é«”ç”¨é‡
    try:
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        if memory_mb > max_mem_mb:
            print(f"â™»ï¸ [PID {os.getpid()}] è¨˜æ†¶é«”è¶…æ¨™ ({memory_mb:.1f} MB > {max_mem_mb} MB)ï¼Œè‡ªå‹•å›æ”¶ worker process")
            sys.exit()  # è‡ªæ®ºï¼Œè®“ Pool å•Ÿå‹•æ–°çš„ä¹¾æ·¨é€²ç¨‹
            
    except Exception as e:
        print(f"âš ï¸ [PID {os.getpid()}] è¨˜æ†¶é«”æª¢æŸ¥å¤±æ•—: {e}")

    # æ²’è¶…æ¨™ï¼Œæ‰åŸ·è¡Œçˆ¬èŸ²ä»»å‹™
    return asyncio.run(_async_crawl_worker(site_config))


def auto_shutdown_vm():
    """
    è‡ªå‹•é—œé–‰ GCE VM åŸ·è¡Œå€‹é«”
    """
    try:
        import subprocess
        
        # ç›´æ¥ä½¿ç”¨å›ºå®šçš„ VM åç¨±å’Œå€åŸŸ
        vm_name = "crawler-webcheck-mpstandard"
        zone = "asia-east1-c"
        
        print(f"ğŸ‰ ä»»å‹™å…¨éƒ¨å®Œæˆï¼Œæº–å‚™è‡ªå‹•é—œé–‰ VM: {vm_name}")
        print(f"ğŸ“ VM ä½ç½®: {zone}")
        
        # åŸ·è¡Œé—œæ©ŸæŒ‡ä»¤
        shutdown_cmd = f"gcloud compute instances stop {vm_name} --zone={zone} --quiet"
        print(f"ğŸ’» åŸ·è¡Œé—œæ©ŸæŒ‡ä»¤: {shutdown_cmd}")
        
        shutdown_result = subprocess.run(
            shutdown_cmd.split(),
            capture_output=True, text=True, timeout=60
        )
        
        if shutdown_result.returncode == 0:
            print("âœ… VM é—œæ©ŸæŒ‡ä»¤åŸ·è¡ŒæˆåŠŸ")
        else:
            print(f"âŒ VM é—œæ©ŸæŒ‡ä»¤åŸ·è¡Œå¤±æ•—: {shutdown_result.stderr}")
            
    except Exception as e:
        print(f"âš ï¸ è‡ªå‹•é—œæ©Ÿå¤±æ•—: {e}")
        print("â„¹ï¸ VM å°‡ä¿æŒé–‹å•Ÿç‹€æ…‹")


def main():
    """
    ä¸»å‡½æ•¸
    """
    parser = argparse.ArgumentParser(description='ç¶²ç«™çˆ¬èŸ²å’Œåˆ†æå·¥å…· (Multiprocessing ç‰ˆæœ¬)')
    parser.add_argument('--depth', type=int, default=2, 
                       help='çˆ¬èŸ²çš„æœ€å¤§æ·±åº¦ (é è¨­: 2)')
    parser.add_argument('--config', type=str, default="config/websites.csv",
                       help='ç¶²ç«™è¨­å®šæª”æ¡ˆè·¯å¾‘ (é è¨­: config/websites.csv)')
    parser.add_argument('--concurrent', type=int, default=2,
                       help='åŒæ™‚è™•ç†çš„ç¶²ç«™æ•¸é‡ (é è¨­: 2)')
    parser.add_argument('--no-save-html', action='store_true',
                       help='ä¸å„²å­˜HTMLæª”æ¡ˆï¼Œåƒ…ç”¢ç”Ÿçµ±è¨ˆå ±å‘Š (æå‡æ•ˆèƒ½ï¼Œç¯€çœç£ç¢Ÿç©ºé–“)')
    parser.add_argument('--no-pagination', action='store_true',
                       help='ç¦ç”¨åˆ†é çˆ¬å–ï¼Œå°‡æœ‰åˆ†é åƒæ•¸çš„é é¢è¦–ç‚ºé‡è¤‡é é¢è·³é (æå‡æ•ˆèƒ½)')
    parser.add_argument('--max-mem-mb', type=int, default=1024,
                       help='subprocess è¨˜æ†¶é«”ä¸Šé™ (MB)ï¼Œè¶…éæ­¤å€¼å°‡è‡ªå‹•å›æ”¶ (é è¨­: 1024)')
    
    args = parser.parse_args()
    
    # å–å¾—å…¨åŸŸé è¨­å€¼
    global_depth = args.depth
    global_save_html = not args.no_save_html
    global_enable_pagination = not args.no_pagination
    
    if not os.path.exists(args.config):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¨­å®šæª”æ¡ˆ {args.config}")
        sys.exit(1)
    
    websites = load_websites(args.config)
    print(f"è¼‰å…¥äº† {len(websites)} å€‹ç¶²ç«™ï¼Œæœ€å¤§çˆ¬èŸ²æ·±åº¦: {global_depth}ï¼Œä¸¦è¡Œæ•¸é‡: {args.concurrent}")
    if global_save_html:
        print("ğŸ’¾ HTMLæª”æ¡ˆå„²å­˜: å•Ÿç”¨")
    else:
        print("ğŸš€ HTMLæª”æ¡ˆå„²å­˜: åœç”¨ (åƒ…ç”¢ç”Ÿçµ±è¨ˆå ±å‘Šï¼Œæå‡æ•ˆèƒ½)")
    
    if global_enable_pagination:
        print("ğŸ“„ åˆ†é çˆ¬å–: å•Ÿç”¨")
    else:
        print("âš¡ åˆ†é çˆ¬å–: åœç”¨ (åˆ†é è¦–ç‚ºé‡è¤‡é é¢è·³éï¼Œæå‡æ•ˆèƒ½)")

    print("ğŸš€ å•Ÿå‹• Multiprocessing ç¶²ç«™çˆ¬èŸ²...")

    # 1. Mainprocess åˆå§‹åŒ– Reporter
    reporter = ReportGenerationAgent()
    output_path = reporter.initialize_excel_report()
    print(f"Excel å ±å‘Šæª”æ¡ˆåˆå§‹åŒ–å®Œæˆ: {output_path}")
    
    processed_urls = reporter.get_processed_urls()
    
    # 2. æº–å‚™è¦å‚³éçµ¦å­é€²ç¨‹çš„ä»»å‹™åˆ—è¡¨
    websites_to_process = []
    for site in websites:
        url = site["URL"]
        if url.strip() in processed_urls:
            continue
            
        # åƒæ•¸åˆä½µè™•ç†  
        try:
            # è®€å–è¡¨æ ¼å…§çš„ depth å€¼
            csv_depth = int(site.get('depth')) if site.get('depth') else None
            # åªæœ‰ç•¶å…¨åŸŸæ·±åº¦å¤§æ–¼è¡¨æ ¼å…§æ·±åº¦æ™‚ï¼Œæ‰ä½¿ç”¨è¡¨æ ¼å…§çš„è¼ƒå°å€¼
            if csv_depth is not None and global_depth > csv_depth:
                site_depth = csv_depth
            else:
                site_depth = global_depth
        except (ValueError, TypeError):
            site_depth = global_depth
        site['global_depth'] = site_depth
        
        # å˜—è©¦è®€å– 'save_html'
        if site.get('save_html', '').lower() == 'true':
            site['global_save_html'] = True
        elif site.get('save_html', '').lower() == 'false':
            site['global_save_html'] = False
        else:
            site['global_save_html'] = global_save_html  # CSV ä¸­ç‚ºç©ºï¼Œä½¿ç”¨å…¨åŸŸè¨­å®š
        
        # å˜—è©¦è®€å– 'pagination'
        if site.get('pagination', '').lower() == 'true':
            site['global_enable_pagination'] = True
        elif site.get('pagination', '').lower() == 'false':
            site['global_enable_pagination'] = False
        else:
            site['global_enable_pagination'] = global_enable_pagination  # CSV ä¸­ç‚ºç©ºï¼Œä½¿ç”¨å…¨åŸŸè¨­å®š
        
        site['global_max_mem_mb'] = args.max_mem_mb
            
        websites_to_process.append(site)
    
    print(f"ğŸ“‹ ç¸½å…± {len(websites)} å€‹ç¶²ç«™ï¼Œå‰©é¤˜ {len(websites_to_process)} å€‹å¾…è™•ç†")
    
    if not websites_to_process:
        print("ğŸ‰ æ‰€æœ‰ç¶²ç«™éƒ½å·²è™•ç†å®Œæˆï¼")
        reporter.finalize_excel_report()
        print(f"ğŸ“„ å ±å‘Šå·²å„²å­˜åˆ°: {output_path}")
        
        # é—œæ©Ÿé‚è¼¯
        auto_shutdown_vm()
        return

    # 3. ä½¿ç”¨ multiprocessing.Pool + memory ç®¡ç†
    print(f"\nğŸš€ å•Ÿå‹• {args.concurrent} å€‹ä¸¦è¡Œè™•ç†ç¨‹åºï¼Œæ¯å€‹è¨˜æ†¶é«”ä¸Šé™ {args.max_mem_mb} MB")
    start_time = time.time()
    
    crawl_success = True
    successful_sites = 0
    failed_sites = 0
    
    try:
        # å»ºç«‹ä¸€å€‹ Pool
        with multiprocessing.Pool(processes=args.concurrent) as pool:

            # ä½¿ç”¨ imap_unordered ä¾†å³æ™‚å–å¾— worker çµæœ
            results = pool.imap_unordered(run_crawl_task, websites_to_process)
            
            # Main process æ¥æ”¶å¾ sub process å‚³å›çš„ã€Œå°å­—å…¸ã€
            for stats_for_excel in results:
                if stats_for_excel:
                    # Main process å‘¼å« add_site_to_excel å¯«å…¥çµæœ
                    try:
                        # åœ¨ Main process ä¸­è¨˜éŒ„çˆ¬å–æ—¥æœŸ
                        crawl_date = datetime.now().strftime('%Y-%m-%d %H:%M')
                        stats_for_excel['crawl_date'] = crawl_date
                        
                        reporter.add_site_to_excel(stats_for_excel)
                        successful_sites += 1
                    except Exception as e:
                        print(f"âŒ å¯«å…¥ Excel å¤±æ•—: {e}")
                        failed_sites += 1
                else:
                    failed_sites += 1

        total_duration = time.time() - start_time
        total_duration_formatted = f"{int(total_duration // 60)}åˆ†{int(total_duration % 60)}ç§’"
        
        print(f"\nğŸ‰ ä¸¦è¡Œè™•ç†å®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸè™•ç†: {successful_sites} å€‹ç¶²ç«™")
        print(f"âŒ å¤±æ•—: {failed_sites} å€‹ç¶²ç«™") 
        print(f"â±ï¸ ç¸½è€—æ™‚: {total_duration_formatted}")

    except Exception as e:
        print(f"\nğŸ’¥ è™•ç†éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        crawl_success = False

    finally:
        # Main process å®Œæˆå ±å‘Š
        reporter.finalize_excel_report()
        print(f"\nğŸ“„ å ±å‘Šå·²å„²å­˜åˆ°: {output_path}")
        print(f"âœ… ç¸½å…±è™•ç†äº† {len(websites_to_process)} å€‹æ–°ç¶²ç«™")
        
        # Main process å‘¼å«é—œæ©Ÿ
        if crawl_success:
            print("æº–å‚™é—œæ©Ÿ...")
            # ç›´æ¥å‘¼å«åŒæ­¥å‡½æ•¸
            auto_shutdown_vm()
        else:
            print("ğŸ”§ ç”±æ–¼åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼ŒVM å°‡ä¿æŒé–‹å•Ÿç‹€æ…‹ä»¥ä¾¿é™¤éŒ¯")


if __name__ == "__main__":
    # ç¢ºä¿ multiprocessing åœ¨ macOS/Windows ä¸Šæ­£å¸¸é‹ä½œ
    multiprocessing.freeze_support() 
    main()  # ç›´æ¥å‘¼å«åŒæ­¥çš„ main