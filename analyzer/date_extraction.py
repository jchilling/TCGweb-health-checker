"""
This module provides functions for extracting and normalizing date information from HTML content.
"""
import re
from datetime import datetime
from bs4 import BeautifulSoup, Tag

# é ç·¨è­¯æ­£å‰‡è¡¨é”å¼ä»¥æå‡æ•ˆèƒ½
_COMPILED_KEYWORD_PATTERNS = [
    re.compile(r'(?:æ›´æ–°æ—¥æœŸ|ç™¼å¸ƒæ—¥æœŸ|ä¿®æ”¹æ—¥æœŸ|ä¸Šç‰ˆæ—¥æœŸ|ä¸Šæ¶æ—¥æœŸ|ç™¼ä½ˆæ—¥æœŸ|å»ºæª”æ—¥æœŸ|æœ€å¾Œæ›´æ–°|è³‡æ–™æ›´æ–°|å…§å®¹æ›´æ–°|è³‡æ–™æª¢è¦–|Data update|Review Date)[:ï¼š\s]*(\d{2,4})(?:å¹´|[/\-\.])(\d{1,2})(?:æœˆ|[/\-\.])(\d{1,2})(?:[æ—¥è™Ÿ])?'),
    re.compile(r'(?:æ›´æ–°æ—¥æœŸ|ç™¼å¸ƒæ—¥æœŸ|ä¿®æ”¹æ—¥æœŸ|ä¸Šç‰ˆæ—¥æœŸ|ä¸Šæ¶æ—¥æœŸ|ç™¼ä½ˆæ—¥æœŸ|å»ºæª”æ—¥æœŸ|æœ€å¾Œæ›´æ–°|è³‡æ–™æ›´æ–°|å…§å®¹æ›´æ–°|è³‡æ–™æª¢è¦–|Data update|Review Date)[:ï¼š\s]*(\d{2,4})(?:å¹´|[/\-\.])(\d{1,2})æœˆ?(?![/\-\.]\d)(?!\d)'),
    re.compile(r'(\d{2,4})(?:å¹´|[/\-\.])(\d{1,2})(?:æœˆ|[/\-\.])(\d{1,2})(?:[æ—¥è™Ÿ])?\s*(?:æ›´æ–°|ç™¼å¸ƒ|ä¿®æ”¹|ç™¼ä½ˆ)'),
    re.compile(r'(\d{2,4})(?:å¹´|[/\-\.])(\d{1,2})æœˆ?(?![/\-\.])\s*(?:æ›´æ–°|ç™¼å¸ƒ|ä¿®æ”¹|ç™¼ä½ˆ)')
]

_COMPILED_GENERIC_PATTERNS = [
    re.compile(r"(?<![\d+*/=.:;@#$%^&|\\])(\d{2,4})(?:å¹´|[/\-\.])(\d{1,2})(?:æœˆ|[/\-\.])(\d{1,2})(?:[æ—¥è™Ÿ])?(?!\d)"),
    re.compile(r"(?<![\d~\-+*/=.:;@#$%^&|\\])(\d{2,4})(?:å¹´|[/\-\.])(0[1-9]|1[0-2]|[1-9])(?![/\.\æœˆ]\d)(?!\d|Â°)"),
    re.compile(r"(?<![\d+*/=.:;@#$%^&|\\])(\d{1,2})[/\-\.](\d{1,2})[/\-\.](\d{4})"),
    re.compile(r"(?<![\d+*/=.:;@#$%^&|\\])(\d{1,2})[/\-\.]((?:19|20)\d{2})(?!\d)")
]

def _clean_html_noise(soup: BeautifulSoup) -> BeautifulSoup:
    """
    ç§»é™¤HTMLä¸­çš„é›œè¨Šå…ƒç´ ï¼Œè¿”å›æ¸…ç†å¾Œçš„HTML
    """
    cleaned_soup = BeautifulSoup(str(soup), 'html.parser')
    
    # è¦ç§»é™¤çš„æ¨™ç±¤é¡å‹
    noise_tags = ['header', 'nav', 'aside', 'footer']
    
    # ç§»é™¤é›œè¨Šæ¨™ç±¤
    for tag_name in noise_tags:
        for tag in cleaned_soup.find_all(tag_name):
            tag.decompose()
    
    # è¦ç§»é™¤çš„CSSé¡åæ¨¡å¼
    noise_class_patterns = [
        'base-footer', 'site-footer', 'footer-container', 'footer-wrapper', 
        'footer-bottom', 'site-info', 'colophon', 'copyright', 'update-time', 
        'visit-count', 'nav', 'navigation', 'navbar', 'nav-menu', 'main-nav', 
        'site-nav', 'breadcrumb', 'sidebar', 'menu', 'top-menu'
    ]
    
    # ç§»é™¤å…·æœ‰é›œè¨Šé¡åçš„å…ƒç´ 
    elements_to_remove = []  # å…ˆæ”¶é›†è¦ç§»é™¤çš„å…ƒç´ 
    for element in cleaned_soup.find_all(attrs={'class': True}):
        if element is None:  # é˜²æ­¢ None å…ƒç´ 
            continue
            
        class_names = element.get('class', [])
        if isinstance(class_names, str):
            class_names = [class_names]
        
        class_str = ' '.join(class_names).lower()
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•é›œè¨Šé¡å
        for pattern in noise_class_patterns:
            if pattern in class_str:
                elements_to_remove.append(element)
                break
    
    # æ‰¹é‡ç§»é™¤å…ƒç´ ä»¥é¿å…è¿­ä»£éç¨‹ä¸­ä¿®æ”¹DOMçš„å•é¡Œ
    for element in elements_to_remove:
        if element and element.parent:  # ç¢ºä¿å…ƒç´ ä»å­˜åœ¨ä¸”æœ‰çˆ¶å…ƒç´ 
            element.decompose()
    
    return cleaned_soup

def _normalize_date_string(groups: tuple) -> str:
    """
    æ ¹æ“šæ­£å‰‡è¡¨é”å¼åŒ¹é…çš„ groups æ­£è¦åŒ–æ—¥æœŸæ ¼å¼ç‚º 'YYYY-MM-DD'
    Args:
        groups: æ­£å‰‡è¡¨é”å¼åŒ¹é…çš„åˆ†çµ„å…ƒçµ„
    Returns:
        æ¨™æº–åŒ–çš„æ—¥æœŸå­—ç¬¦ä¸² 'YYYY-MM-DD'ï¼Œå¦‚æœæ—¥æœŸå¤ªèˆŠå‰‡è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not groups:
        return ""
    
    # è½‰æ›ç‚ºæ•´æ•¸åˆ—è¡¨ï¼Œéæ¿¾ç©ºå€¼
    nums = [int(g) for g in groups if g and g.isdigit()]
    
    if len(nums) == 3:
        # ä¸‰å€‹æ•¸å­—ï¼šå¹´æœˆæ—¥
        a, b, c = nums
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºæ—¥æœˆå¹´æ ¼å¼ï¼ˆæœ€å¾Œä¸€å€‹æ˜¯å››ä½æ•¸è¥¿å…ƒå¹´ï¼‰
        if c >= 1900:
            # æ—¥æœˆå¹´æ ¼å¼
            day, month, year = a, b, c
            # è¥¿å…ƒå¹´å¿…é ˆåœ¨1990å¹´ä»¥å¾Œ
            if year < 1990:
                return ""
        else:
            # å¹´æœˆæ—¥æ ¼å¼
            year, month, day = a, b, c
            # å¦‚æœå¹´ä»½å°æ–¼200ï¼Œè¦–ç‚ºæ°‘åœ‹å¹´
            if year < 200:
                # æ°‘åœ‹å¹´å¿…é ˆåœ¨79å¹´ä»¥å¾Œï¼ˆå°æ‡‰è¥¿å…ƒ1990å¹´ï¼‰
                if year < 79:
                    return ""
                year += 1911
            else:
                # è¥¿å…ƒå¹´å¿…é ˆåœ¨1990å¹´ä»¥å¾Œ
                if year < 1990:
                    return ""
        
        return f"{year:04d}-{month:02d}-{day:02d}"
    
    elif len(nums) == 2:
        # å…©å€‹æ•¸å­—ï¼šå¹´æœˆ
        a, b = nums
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºæœˆå¹´æ ¼å¼ï¼ˆæœ€å¾Œä¸€å€‹æ˜¯å››ä½æ•¸è¥¿å…ƒå¹´ï¼‰
        if b >= 1900:
            # æœˆå¹´æ ¼å¼
            month, year = a, b
            # è¥¿å…ƒå¹´å¿…é ˆåœ¨1990å¹´ä»¥å¾Œ
            if year < 1990:
                return ""
        else:
            # å¹´æœˆæ ¼å¼
            year, month = a, b
            # å¦‚æœå¹´ä»½å°æ–¼200ï¼Œè¦–ç‚ºæ°‘åœ‹å¹´
            if year < 200:
                # æ°‘åœ‹å¹´å¿…é ˆåœ¨79å¹´ä»¥å¾Œï¼ˆå°æ‡‰è¥¿å…ƒ1990å¹´ï¼‰
                if year < 79:
                    return ""
                year += 1911
            else:
                # è¥¿å…ƒå¹´å¿…é ˆåœ¨1990å¹´ä»¥å¾Œ
                if year < 1990:
                    return ""
        
        # é è¨­ç‚ºè©²æœˆç¬¬ä¸€å¤©
        return f"{year:04d}-{month:02d}-01"
    
    # ç„¡æ³•è§£æ
    return ""

def _search_for_date_in_scope(scope: Tag, scope_name: str = "unknown", log_func=None) -> tuple[list[str], bool]:
    """Searches for dates within a specific BeautifulSoup scope (tag).
    Returns a tuple of (found_dates, used_generic_patterns)."""
    if not scope:
        return [], False

    found_dates = []
    used_generic_patterns = False

    def _log(message):
        if log_func:
            log_func(message)
        else:
            print(message)

    keyword_matches = []  # é—œéµè©æ¨¡å¼çš„åŒ¹é…çµæœ
    generic_matches = []  # é€šç”¨æ ¼å¼çš„åŒ¹é…çµæœ
    
    # æ‰¹é‡æå–æ‰€æœ‰æ–‡æœ¬å…ƒç´ ï¼Œé¿å…é‡è¤‡éæ­·DOM
    text_elements = []
    for element in scope.find_all(string=True):
        if element.parent and element.strip():
            text_elements.append(element.strip())
    
    for text_content in text_elements:
        # æ”¶é›†é—œéµè©æ¨¡å¼åŒ¹é…
        for i, compiled_pattern in enumerate(_COMPILED_KEYWORD_PATTERNS):
            matches = compiled_pattern.finditer(text_content)
            for match in matches:
                # å‚³å…¥æ‰€æœ‰æ•ç²çš„åˆ†çµ„ï¼ˆæ’é™¤ç¬¬0çµ„å®Œæ•´åŒ¹é…ï¼‰
                date_groups = match.groups()
                date_str = _normalize_date_string(date_groups)
                if date_str and date_str not in found_dates:
                    keyword_matches.append((i+1, date_str, str(date_groups), match.group(0)))
        
        # æ”¶é›†é€šç”¨æ ¼å¼åŒ¹é…
        for i, compiled_pattern in enumerate(_COMPILED_GENERIC_PATTERNS):
            matches = compiled_pattern.finditer(text_content)
            for match in matches:
                # å‚³å…¥æ‰€æœ‰æ•ç²çš„åˆ†çµ„ï¼ˆæ’é™¤ç¬¬0çµ„å®Œæ•´åŒ¹é…ï¼‰
                date_groups = match.groups()
                date_str = _normalize_date_string(date_groups)
                if date_str and date_str not in found_dates:
                    generic_matches.append((i+1, date_str, str(date_groups)))
    
    # æ ¹æ“šå„ªå…ˆç´šè™•ç†çµæœ
    if keyword_matches:
        # æœ‰é—œéµè©åŒ¹é…ï¼Œä½¿ç”¨é—œéµè©çµæœ
        for pattern_num, date_str, original, full_match in keyword_matches:
            if date_str not in found_dates:
                _log(f"ğŸ¯ æ‰¾åˆ°æ—¥æœŸ: {date_str} (ä¾†æº: é—œéµè©, åŸå§‹: {original})")
                found_dates.append(date_str)
    else:
        # æ²’æœ‰é—œéµè©åŒ¹é…ï¼Œä½¿ç”¨é€šç”¨æ ¼å¼çµæœï¼Œä¸¦æ¨™è¨˜å·²ä½¿ç”¨é€šç”¨æ ¼å¼
        used_generic_patterns = True
        if generic_matches:
            for pattern_num, date_str, original in generic_matches:
                if date_str not in found_dates:
                    _log(f"ğŸ“… æ‰¾åˆ°æ—¥æœŸ: {date_str} (ä¾†æº: é€šç”¨æ ¼å¼, åŸå§‹: {original})")
                    found_dates.append(date_str)
                            
    return found_dates, used_generic_patterns

def _select_best_date(dates: list[str], log_func=None) -> str:
    """
    å¾å¤šå€‹æ—¥æœŸä¸­é¸æ“‡æœ€åˆé©çš„ä¸€å€‹ä½œç‚ºç¶²ç«™æœ€å¾Œæ›´æ–°æ—¥æœŸ
    ç­–ç•¥ï¼š
    1. å„ªå…ˆé¸æ“‡æœ€è¿‘çš„æ—¥æœŸï¼ˆé€šå¸¸æ˜¯æœ€å¾Œæ›´æ–°æ—¥æœŸï¼‰
    2. æ’é™¤æœªä¾†æ—¥æœŸ
    3. å¦‚æœæ²’æœ‰æ‰¾åˆ°ä»»ä½•æ—¥æœŸï¼Œè¿”å› "[ç„¡æ—¥æœŸ]"
    """
    
    def _log(message):
        if log_func:
            log_func(message)
        else:
            print(message)
    
    if not dates:
        _log(f"  âŒ ç„¡æ³•æ‰¾åˆ°æœ‰æ•ˆæ—¥æœŸï¼Œè¿”å›ç„¡æ—¥æœŸ")
        return "[ç„¡æ—¥æœŸ]"
    
    if len(dates) == 1:
        _log(f"  âœ… åªæœ‰ä¸€å€‹æ—¥æœŸï¼Œç›´æ¥é¸æ“‡: {dates[0]}")
        return dates[0]
    
    # éæ¿¾å’Œæ’åºæ—¥æœŸï¼ŒåŒæ™‚è¨˜éŒ„æœ€æ¥è¿‘çš„æ—¥æœŸ
    valid_dates = []
    closest_date = None
    closest_diff = None
    current_obj = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    
    for date_str in dates:
        try:
            # æª¢æŸ¥æ—¥æœŸæ ¼å¼
            if re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
                date_obj = datetime.strptime(date_str, "%Y-%m-%d") 
                
                # è¨ˆç®—èˆ‡ç•¶å‰æ—¥æœŸçš„æ™‚é–“å·®ï¼Œè¨˜éŒ„æœ€æ¥è¿‘çš„æ—¥æœŸ
                diff = abs((date_obj - current_obj).days)
                if closest_diff is None or diff < closest_diff:
                    closest_diff = diff
                    closest_date = date_str
                
                # æ’é™¤æœªä¾†æ—¥æœŸå’Œç•¶å¤©æ—¥æœŸ
                if date_obj >= current_obj:
                    continue
                
                valid_dates.append(date_str)
        except ValueError:
            continue
    
    if not valid_dates:
        if closest_date:
            return closest_date
        else:
            return "[ç„¡æ—¥æœŸ]"
    
    # è¿”å›æœ€è¿‘çš„æ—¥æœŸ
    best_date = max(valid_dates)
    _log(f" ğŸ† æœ€çµ‚é¸æ“‡çš„æ—¥æœŸ: {best_date}")
    return best_date


def extract_last_updated(soup: BeautifulSoup, log_func=None) -> str:
    """
    Extracts the last updated date from a BeautifulSoup object using a hierarchical and semantic strategy.
    ç•¶ç¶²é æœ‰å¤šå€‹æ—¥æœŸæ™‚ï¼Œæœƒé¸æ“‡æœ€åˆé©çš„ä¸€å€‹ã€‚
    
    1. é¦–å…ˆæ¸…ç†HTMLä¸­çš„é›œè¨Šå…ƒç´ ï¼ˆheader, footer, navç­‰ï¼‰
    2. åœ¨æ¸…ç†å¾Œçš„HTMLä¸­æœå°‹æ—¥æœŸ
    3. å¦‚æœä½¿ç”¨äº†é€šç”¨æ ¼å¼æ¨¡å¼ï¼Œå‰‡æª¢æŸ¥metaæ¨™ç±¤ä½œç‚ºè£œå……
    """
    def _log(message):
        if log_func:
            log_func(message)
        else:
            print(message)
    
    all_found_dates = []
    used_generic_patterns = False
    
    # æ¸…ç†HTMLé›œè¨Š
    cleaned_soup = _clean_html_noise(soup)
    
    # å…ˆå˜—è©¦åœ¨æ¸…ç†å¾Œçš„ body ä¸­æœå°‹
    cleaned_body = cleaned_soup.find('body')
    if cleaned_body:
        # åœ¨æ¸…ç†å¾Œçš„ body ä¸­æœå°‹æ—¥æœŸ
        scope_dates, scope_used_generic = _search_for_date_in_scope(cleaned_body, 'cleaned body', log_func)
        if scope_used_generic:
            used_generic_patterns = True
        for date in scope_dates:
            if date and date not in all_found_dates:
                all_found_dates.append(date)
    else:
        # å¦‚æœæ²’æœ‰æ‰¾åˆ° bodyï¼Œå‰‡åœ¨æ•´å€‹æ¸…ç†å¾Œçš„æ–‡æª”ä¸­æœå°‹
        scope_dates, scope_used_generic = _search_for_date_in_scope(cleaned_soup, 'cleaned entire document', log_func)
        if scope_used_generic:
            used_generic_patterns = True
        for date in scope_dates:
            if date and date not in all_found_dates:
                all_found_dates.append(date)

    # 2. Check meta tags only if generic patterns were used
    if used_generic_patterns:
        meta_properties = [
            'og:article:modified_time', 'og:modified_time', 'article:modified_time',
            'og:article:published_time', 'og:published_time', 'article:published_time',
            'DC.date.modified', 'dcterms.modified', 'DC.Date', 'dcterms.created',
            'DC.Coverage.t.min', 'DC.Coverage.t.max'
        ]
        for prop in meta_properties:
            meta_tag = soup.find('meta', property=prop) or soup.find('meta', attrs={'name': prop})
            if meta_tag and meta_tag.get('content'):
                content = meta_tag['content'].strip()
                # ç›´æ¥ç”¨ - åˆ†å‰²è™•ç† YYYY-MM-DD æˆ– YYYY-MM æ ¼å¼
                parts = content.split('-')
                if len(parts) >= 2:  # è‡³å°‘æœ‰å¹´æœˆ
                    # æª¢æŸ¥å‰å…©å€‹éƒ¨åˆ†æ˜¯å¦ç‚ºæ•¸å­—ä¸”å¹´ä»½ç‚º4ä½æ•¸
                    if (len(parts[0]) == 4 and parts[0].isdigit() and 
                        parts[1].isdigit()):
                        date_groups = tuple(parts[:3])  # æœ€å¤šå–å‰3å€‹éƒ¨åˆ†ï¼ˆå¹´æœˆæ—¥ï¼‰
                        date_str = _normalize_date_string(date_groups)
                        if date_str and date_str not in all_found_dates:
                            _log(f"ğŸ·ï¸ æ‰¾åˆ°æ—¥æœŸ: {date_str} (ä¾†æº: metaæ¨™ç±¤, åŸå§‹: {content})")
                            all_found_dates.append(date_str)
    
    # Select the best date from all found dates
    result = _select_best_date(all_found_dates, log_func)
    return result
