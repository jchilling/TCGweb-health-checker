import csv
import os
import sys
import asyncio
import argparse
import time
import psutil
import multiprocessing
from multiprocessing import Process, Queue
import subprocess
from queue import Empty 
from datetime import datetime, timedelta
from playwright.async_api import async_playwright
import zipfile
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

from crawler.web_crawler import WebCrawlerAgent
from reporter.report_generation_mp import ReportGenerationAgent
from utils.extract_problematic_links import extract_error_links_from_json


def load_websites(path: str):
    """è¼‰å…¥ç¶²ç«™è¨­å®šæª”"""
    websites_config = []
    with open(path, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            websites_config.append(row)
    return websites_config


async def _async_crawl_worker(site_config: dict) -> dict:
    """
    Subprocess ä¸­ asyncio è¿´åœˆå…§åŸ·è¡Œçš„çœŸæ­£çˆ¬èŸ²
    """
    url = site_config["URL"]
    name = site_config.get("name", "")
    depth = site_config["global_depth"]
    save_html = site_config["global_save_html"]
    enable_pagination = site_config["global_enable_pagination"]

    print(f"\nğŸ” [PID {os.getpid()}] é–‹å§‹è™•ç†ç¶²ç«™: {name or url}")
    
    try:
        # åœ¨ subprocess ä¸­å»ºç«‹ crawler å’Œ playwright
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            crawler = WebCrawlerAgent(save_html_files=save_html, enable_pagination=enable_pagination)
            
            start_time = time.time()
            
            # åŸ·è¡Œçˆ¬èŸ²
            crawl_results = await crawler.crawl_site(browser, url, name=name, max_depth=depth)
            crawl_duration = time.time() - start_time
            crawl_duration_formatted = f"{int(crawl_duration // 60)}åˆ†{int(crawl_duration % 60)}ç§’"
            
            page_summary = crawler.get_page_summary()
            external_link_results = crawler.get_external_link_results()

            # å„²å­˜ JSON/Log
            json_path = crawler.save_page_summary_to_json()
            if json_path:
                extract_error_links_from_json(json_path)
            crawler.save_crawl_log()

            # é å…ˆè¨ˆç®—çµ±è¨ˆæ•¸æ“šçµ¦ Excel
            one_year_ago = datetime.now() - timedelta(days=365)
            total_pages = len(crawl_results)
            failed_pages = sum(1 for status in crawl_results if status >= 400 or status == 0)
            failed_external_links = sum(1 for link_info in external_link_results.values() 
                                       if link_info.get('status', 0) >= 400 or link_info.get('status', 0) == 0)
            
            # è¨ˆç®—æ—¥æœŸç›¸é—œçµ±è¨ˆ
            today = datetime.now().date()
            pages_with_date = 0
            no_date_pages = 0
            outdated_pages = 0
            past_dates = []  # ä»Šå¤©æˆ–ä»¥å‰çš„æ—¥æœŸ
            future_dates = []  # æœªä¾†æ—¥æœŸ
            
            for url_key, page_info in page_summary.items():
                last_updated = page_info.get('last_updated', '')
                
                # çµ±è¨ˆç„¡æ—¥æœŸçš„é é¢
                if last_updated == "[ç„¡æ—¥æœŸ]" or last_updated == "[çˆ¬å–å¤±æ•—]" or not last_updated:
                    no_date_pages += 1
                    continue
                
                try:
                    update_date = datetime.strptime(last_updated, '%Y-%m-%d')
                    update_date_only = update_date.date()
                    
                    if update_date_only <= today:
                        past_dates.append(update_date)
                        # æª¢æŸ¥æ˜¯å¦ç‚ºä¸€å¹´å‰çš„å…§å®¹
                        if update_date < one_year_ago:
                            outdated_pages += 1
                    else:
                        future_dates.append(update_date)
                        
                except ValueError:
                    no_date_pages += 1
                    continue
            
            # è¨ˆç®—æœ€æ–°æ›´æ–°æ—¥æœŸï¼šå„ªå…ˆä½¿ç”¨éå»æ—¥æœŸçš„æœ€æ–°å€¼ï¼Œæ²’æœ‰æ‰ç”¨æœ€æ¥è¿‘ä»Šå¤©çš„æœªä¾†æ—¥æœŸ
            if past_dates:
                latest_update = max(past_dates).strftime('%Y-%m-%d')
            elif future_dates:
                latest_update = min(future_dates).strftime('%Y-%m-%d')
            else:
                latest_update = "ç„¡æœ‰æ•ˆæ—¥æœŸ"
            
            # è¨ˆç®—ä¸€å¹´å‰å…§å®¹çš„æ¯”ä¾‹
            pages_with_date = len(past_dates) + len(future_dates)
            outdated_percentage = (outdated_pages / pages_with_date * 100) if pages_with_date > 0 else 0
            
            # å»ºç«‹å°å‹çµæœå­—å…¸
            stats_for_excel = {
                'site_name': name or url,
                'site_url': url,
                'total_pages': total_pages,
                'pages_with_date': pages_with_date,
                'no_date_pages': no_date_pages,
                'latest_update': latest_update,
                'outdated_pages': outdated_pages,
                'outdated_percentage': round(outdated_percentage, 2),
                'failed_pages': failed_pages,
                'failed_external_links': failed_external_links,
                'total_external_links': len(external_link_results),
                'crawl_duration': crawl_duration_formatted
            }
            
            # æ¸…ç†ä¸¦é—œé–‰
            del page_summary
            del external_link_results
            await crawler.close()
            crawler.clear_memory()
            del crawler
            await browser.close()
            
            return stats_for_excel
                
    except Exception as e:
        print(f"âŒ [PID {os.getpid()}] è™•ç†ç¶²ç«™ '{name or url}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        try:
            if 'crawler' in locals() and crawler:
                await crawler.close()
                crawler.clear_memory()
                del crawler
            if 'browser' in locals() and browser:
                await browser.close()
            
            # çµ¦äºˆ 5 ç§’ç·©è¡æ™‚é–“
            await asyncio.sleep(5)
            
        except Exception as cleanup_e:
            print(f"ğŸ’¥ [PID {os.getpid()}] åœ¨éŒ¯èª¤æ¸…ç†ä¸­ç™¼ç”Ÿäº†é¡å¤–éŒ¯èª¤: {cleanup_e}")
            
        return None  # ç™¼ç”ŸéŒ¯èª¤æ™‚è¿”å› None


def worker_process_loop(worker_id: int, task_queue: Queue, result_queue: Queue, max_mem_mb: int):
    """
    è‡ªè¨‚çš„ Worker Process è¿´åœˆ
    å®ƒæœƒå…ˆæª¢æŸ¥è¨˜æ†¶é«”ï¼Œå†æ±ºå®šæ˜¯å¦æ¥ä»»å‹™
    """
    print(f"âœ… [Worker {worker_id} | PID {os.getpid()}] å•Ÿå‹•")
    
    process = psutil.Process(os.getpid())
    
    while True:
        try:
            # æ¥ä»»å‹™å‰çš„è¨˜æ†¶é«”æª¢æŸ¥
            memory_mb = process.memory_info().rss / 1024 / 1024
            
            if memory_mb > max_mem_mb:
                print(f"â™»ï¸  [Worker {worker_id} | PID {os.getpid()}] è¨˜æ†¶é«”è¶…æ¨™ ({memory_mb:.1f} MB)ï¼Œè«‹æ±‚é‡å•Ÿ...")
                result_queue.put(("RESTART", worker_id)) 
                break 

            # è¨˜æ†¶é«”æ­£å¸¸ï¼Œå˜—è©¦æ¥ä»»å‹™
            try:
                site_config = task_queue.get(timeout=5.0) 
            except Empty:
                print(f"âŒ› [Worker {worker_id} | PID {os.getpid()}] ä»»å‹™ä½‡åˆ—ç‚ºç©ºï¼Œè‡ªå‹•é€€å‡º")
                break

            # æª¢æŸ¥çµæŸè¨Šè™Ÿ
            if site_config is None:
                # æ”¶åˆ° None è¨Šè™Ÿï¼Œä»£è¡¨ä»»å‹™å·²å…¨éƒ¨æ´¾ç™¼ï¼Œç›´æ¥é€€å‡º
                print(f"ğŸ›‘ [Worker {worker_id} | PID {os.getpid()}] æ”¶åˆ°çµæŸè¨Šè™Ÿï¼Œé€€å‡º")
                break

            # åŸ·è¡Œçˆ¬èŸ²ä»»å‹™
            try:
                stats_for_excel = asyncio.run(_async_crawl_worker(site_config))
                
                # å‚³å›çµæœ
                result_queue.put(stats_for_excel)
                print(f"\nâœ… [Worker {worker_id} | PID {os.getpid()}] ç¶²ç«™ '{site_config.get('name', 'N/A')}' è™•ç†å®Œæˆ")
                
            except Exception as e:
                print(f"ğŸ’¥ [Worker {worker_id} | PID {os.getpid()}] åŸ·è¡Œä»»å‹™ '{site_config.get('name', 'N/A')}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                # å›å ±å¤±æ•—ï¼ŒåŒ…å«ç¶²ç«™è³‡è¨Šä»¥ä¾¿è¿½è¹¤
                result_queue.put(("FAILED", site_config.get('name', 'N/A')))
        
        except Exception as loop_e:
            # æ•æ‰ worker è¿´åœˆæœ¬èº«çš„éŒ¯èª¤
            print(f"ğŸ†˜ [Worker {worker_id} | PID {os.getpid()}] è¿´åœˆç™¼ç”Ÿåš´é‡éŒ¯èª¤: {loop_e}")
            result_queue.put(("RESTART", worker_id)) # ä¹Ÿè«‹æ±‚é‡å•Ÿ
            break
            
    print(f"ğŸ‘‹ [Worker {worker_id} | PID {os.getpid()}] çµæŸ")


def pack_and_send_email(excel_report_path):
    """
    æ‰“åŒ… (Excel + Log + Assets) ä¸¦ç™¼é€ Email
    """
    GMAIL_USER = os.getenv("GMAIL_USER")
    GMAIL_APP_PASSWORD = os.getenv("GMAIL_APP_PASSWORD")
    TO_EMAIL = os.getenv("TO_EMAIL", GMAIL_USER)  # å¦‚æœæ²’è¨­æ”¶ä»¶äººï¼Œé è¨­å¯„çµ¦è‡ªå·±

    if not GMAIL_USER or not GMAIL_APP_PASSWORD:
        print("âŒ éŒ¯èª¤ï¼šæœªè¨­å®š GMAIL_USER æˆ– GMAIL_APP_PASSWORD ç’°å¢ƒè®Šæ•¸")
        print("ğŸ’¡ è«‹ç¢ºèª .env æª”æ¡ˆå·²å»ºç«‹ä¸”åŒ…å«å¿…è¦çš„è¨­å®š")
        return
    
    print(f"ğŸ“§ ä½¿ç”¨ Gmail å¸³è™Ÿ: {GMAIL_USER}")
    print(f"ğŸ“¬ æ”¶ä»¶äºº: {TO_EMAIL}")
    
    # zip æª”å
    timestamp = datetime.now().strftime('%Y%m')
    zip_filename = f"website_check_results_{timestamp}.zip"
    
    try:
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            
            # Excel
            if os.path.exists(excel_report_path):
                print(f"åŠ å…¥å ±å‘Š: {excel_report_path}")
                zipf.write(excel_report_path, os.path.basename(excel_report_path))
            else:
                print(f"âš ï¸ æ‰¾ä¸åˆ°å ±å‘Šæª”: {excel_report_path}")
            
            # crawler_execution.log
            vm_log_path = os.path.expanduser('~/crawler_execution.log')
            if os.path.exists(vm_log_path):
                print(f"åŠ å…¥æ—¥èªŒ: {vm_log_path}")
                zipf.write(vm_log_path, "crawler_execution.log")
            else:
                print(f"âš ï¸ æ‰¾ä¸åˆ°æ—¥èªŒæª”: {vm_log_path}")

            # assets
            if os.path.exists("assets"):
                for root, dirs, files in os.walk("assets"):
                    for file in files:
                        # è·³é html æª”
                        # if file.lower().endswith('.html'):
                            # continue
                        
                        file_path = os.path.join(root, file)
                        # åœ¨ zip ä¸­çš„è·¯å¾‘ (ä¿æŒè³‡æ–™å¤¾çµæ§‹)
                        arcname = os.path.relpath(file_path, start=".")
                        zipf.write(file_path, arcname)
                print(f"åŠ å…¥å„ç¶²ç«™è©³ç´°æª”æ¡ˆ")
            else:
                print(f"âš ï¸ æ‰¾ä¸åˆ° assets è³‡æ–™å¤¾")
            
        print(f"âœ… å£“ç¸®å®Œæˆ: {zip_filename}\n")

        # æª¢æŸ¥ zip æª”æ¡ˆå¤§å°
        zip_size_mb = os.path.getsize(zip_filename) / 1024 / 1024
        
        if zip_size_mb > 25:
            print(f"âš ï¸ è­¦å‘Šï¼š{zip_size_mb:.2f} è¶…é Gmail 25MB é™åˆ¶ï¼Œå¯èƒ½ç„¡æ³•ç™¼é€")

        # é€ Email
        print("ğŸ“§ æ­£åœ¨ç™¼é€ Email...")
        msg = MIMEMultipart()
        msg['Subject'] = f"ç¶²ç«™æª¢æ ¸çˆ¬èŸ²å®Œæ•´è³‡æ–™çµæœ - {datetime.now().strftime('%Y/%m')}"
        msg['From'] = GMAIL_USER
        msg['To'] = TO_EMAIL
        
        body = (
            "çˆ¬èŸ²ä»»å‹™å·²å®Œæˆã€‚\n\n"
            "é™„ä»¶æ‡‰åŒ…å«ï¼š\n"
            "1. Excel çµ±è¨ˆå ±å‘Š\n"
            "2. åŸ·è¡Œæ—¥èªŒ (crawler_execution.log)\n"
            "3. Assets è³‡æ–™å¤¾å…§å®¹\n\n"
            f"å£“ç¸®æª”å¤§å°: {zip_size_mb:.2f} MB\n\n"
            "(ç”± GCP VM è‡ªå‹•ç™¼é€)"
        )
        msg.attach(MIMEText(body, 'plain'))
        
        # Zip é™„ä»¶
        with open(zip_filename, "rb") as f:
            part = MIMEApplication(f.read(), Name=zip_filename)
        
        part['Content-Disposition'] = f'attachment; filename="{zip_filename}"'
        msg.attach(part)
        
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)
            server.send_message(msg)
            print("âœ… Email ç™¼é€æˆåŠŸï¼\n")
            
        # ç™¼é€å¾Œåˆªé™¤ zip æª”ä»¥ç¯€çœç©ºé–“
        os.remove(zip_filename)

    except Exception as e:
        print(f"âŒ æ‰“åŒ…æˆ–ç™¼é€ Email å¤±æ•—: {e}\n")
        # import traceback
        # print(f"è©³ç´°éŒ¯èª¤è³‡è¨Š: {traceback.format_exc()}")


def auto_shutdown_vm():
    """
    è‡ªå‹•é—œé–‰ GCE VM åŸ·è¡Œå€‹é«”
    """
    try:
        # ç›´æ¥ä½¿ç”¨å›ºå®šçš„ VM åç¨±å’Œå€åŸŸ
        vm_name = "crawler-webcheck-mpselfqueue"
        zone = "asia-east1-c"
        
        print(f"ğŸ‰ ä»»å‹™å…¨éƒ¨å®Œæˆï¼Œæº–å‚™è‡ªå‹•é—œé–‰ VM: {vm_name}")
        print(f"ğŸ“ VM ä½ç½®: {zone}")
        
        # åŸ·è¡Œé—œæ©ŸæŒ‡ä»¤
        shutdown_cmd = f"gcloud compute instances stop {vm_name} --zone={zone} --quiet"
        print(f"ğŸ’» åŸ·è¡Œé—œæ©ŸæŒ‡ä»¤: {shutdown_cmd}")
        
        shutdown_result = subprocess.run(
            shutdown_cmd.split(),
            capture_output=True, text=True, timeout=60
        )
        
        if shutdown_result.returncode == 0:
            print("âœ… VM é—œæ©ŸæŒ‡ä»¤åŸ·è¡ŒæˆåŠŸ")
        else:
            print(f"âŒ VM é—œæ©ŸæŒ‡ä»¤åŸ·è¡Œå¤±æ•—: {shutdown_result.stderr}")
            
    except Exception as e:
        print(f"âš ï¸ è‡ªå‹•é—œæ©Ÿå¤±æ•—: {e}")
        print("â„¹ï¸ VM å°‡ä¿æŒé–‹å•Ÿç‹€æ…‹")


def main():
    """
    ä¸»å‡½æ•¸
    """
    parser = argparse.ArgumentParser(description='ç¶²ç«™çˆ¬èŸ²å’Œåˆ†æå·¥å…· (Multiprocessing ç‰ˆæœ¬)')
    parser.add_argument('--depth', type=int, default=2, 
                       help='çˆ¬èŸ²çš„æœ€å¤§æ·±åº¦ (é è¨­: 2)')
    parser.add_argument('--config', type=str, default="config/websites.csv",
                       help='ç¶²ç«™è¨­å®šæª”æ¡ˆè·¯å¾‘ (é è¨­: config/websites.csv)')
    parser.add_argument('--concurrent', type=int, default=2,
                       help='åŒæ™‚è™•ç†çš„ç¶²ç«™æ•¸é‡ (é è¨­: 2)')
    parser.add_argument('--no-save-html', action='store_true',
                       help='ä¸å„²å­˜HTMLæª”æ¡ˆï¼Œåƒ…ç”¢ç”Ÿçµ±è¨ˆå ±å‘Š (æå‡æ•ˆèƒ½ï¼Œç¯€çœç£ç¢Ÿç©ºé–“)')
    parser.add_argument('--no-pagination', action='store_true',
                       help='ç¦ç”¨åˆ†é çˆ¬å–ï¼Œå°‡æœ‰åˆ†é åƒæ•¸çš„é é¢è¦–ç‚ºé‡è¤‡é é¢è·³é (æå‡æ•ˆèƒ½)')
    parser.add_argument('--max-mem-mb', type=int, default=1024,
                       help='subprocess è¨˜æ†¶é«”ä¸Šé™ (MB)ï¼Œè¶…éæ­¤å€¼å°‡è‡ªå‹•å›æ”¶ (é è¨­: 1024)')
    
    args = parser.parse_args()
    
    # å–å¾—å…¨åŸŸé è¨­å€¼
    global_depth = args.depth
    global_save_html = not args.no_save_html
    global_enable_pagination = not args.no_pagination
    
    if not os.path.exists(args.config):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¨­å®šæª”æ¡ˆ {args.config}")
        sys.exit(1)
    
    websites = load_websites(args.config)
    print(f"è¼‰å…¥äº† {len(websites)} å€‹ç¶²ç«™ï¼Œæœ€å¤§çˆ¬èŸ²æ·±åº¦: {global_depth}ï¼Œä¸¦è¡Œæ•¸é‡: {args.concurrent}")
    if global_save_html:
        print("ğŸ’¾ HTMLæª”æ¡ˆå„²å­˜: å•Ÿç”¨")
    else:
        print("ğŸš€ HTMLæª”æ¡ˆå„²å­˜: åœç”¨ (åƒ…ç”¢ç”Ÿçµ±è¨ˆå ±å‘Šï¼Œæå‡æ•ˆèƒ½)")
    
    if global_enable_pagination:
        print("ğŸ“„ åˆ†é çˆ¬å–: å•Ÿç”¨")
    else:
        print("âš¡ åˆ†é çˆ¬å–: åœç”¨ (åˆ†é è¦–ç‚ºé‡è¤‡é é¢è·³éï¼Œæå‡æ•ˆèƒ½)")

    print("ğŸš€ å•Ÿå‹• Multiprocessing ç¶²ç«™çˆ¬èŸ²...")

    # åˆå§‹åŒ– Reporter
    reporter = ReportGenerationAgent()
    output_path = reporter.initialize_excel_report()
    print(f"Excel å ±å‘Šæª”æ¡ˆåˆå§‹åŒ–å®Œæˆ: {output_path}")
    
    processed_urls = reporter.get_processed_urls()
    
    # ä»»å‹™åˆ—è¡¨
    websites_to_process = []
    for site in websites:
        url = site["URL"]
        if url.strip() in processed_urls:
            continue
            
        # åƒæ•¸åˆä½µè™•ç†  
        try:
            # è®€å–è¡¨æ ¼å…§çš„ depth å€¼
            csv_depth = int(site.get('depth')) if site.get('depth') else None
            # åªæœ‰ç•¶å…¨åŸŸæ·±åº¦å¤§æ–¼è¡¨æ ¼å…§æ·±åº¦æ™‚ï¼Œæ‰ä½¿ç”¨è¡¨æ ¼å…§çš„è¼ƒå°å€¼
            if csv_depth is not None and global_depth > csv_depth:
                site_depth = csv_depth
            else:
                site_depth = global_depth
        except (ValueError, TypeError):
            site_depth = global_depth
        site['global_depth'] = site_depth
        
        # å˜—è©¦è®€å– 'save_html'
        if site.get('save_html', '').lower() == 'true':
            site['global_save_html'] = True
        elif site.get('save_html', '').lower() == 'false':
            site['global_save_html'] = False
        else:
            site['global_save_html'] = global_save_html  # CSV ä¸­ç‚ºç©ºï¼Œä½¿ç”¨å…¨åŸŸè¨­å®š
        
        # å˜—è©¦è®€å– 'pagination'
        if site.get('pagination', '').lower() == 'true':
            site['global_enable_pagination'] = True
        elif site.get('pagination', '').lower() == 'false':
            site['global_enable_pagination'] = False
        else:
            site['global_enable_pagination'] = global_enable_pagination  # CSV ä¸­ç‚ºç©ºï¼Œä½¿ç”¨å…¨åŸŸè¨­å®š
        
        site['global_max_mem_mb'] = args.max_mem_mb
            
        websites_to_process.append(site)
    
    num_total_tasks = len(websites_to_process)
    print(f"ğŸ“‹ ç¸½å…± {len(websites)} å€‹ç¶²ç«™ï¼Œå‰©é¤˜ {num_total_tasks} å€‹å¾…è™•ç†")
    
    if not websites_to_process:
        print("ğŸ‰ æ‰€æœ‰ç¶²ç«™éƒ½å·²è™•ç†å®Œæˆï¼")
        reporter.finalize_excel_report()
        print(f"ğŸ“„ å ±å‘Šå·²å„²å­˜åˆ°: {output_path}")
        
        print("æº–å‚™æ‰“åŒ…ä¸¦ç™¼é€å ±å‘Š...")
        pack_and_send_email(output_path)
        
        # é—œæ©Ÿ
        auto_shutdown_vm()
        return

    # å»ºç«‹æ‰‹å‹•çš„ Process å’Œ Queue
    
    task_queue = Queue()
    result_queue = Queue()
    
    for site_config in websites_to_process:
        task_queue.put(site_config)
    
    # æ”¾å…¥çµæŸè¨Šè™Ÿï¼Œç­‰æ–¼ worker æ•¸çš„ None
    for _ in range(args.concurrent):
        task_queue.put(None)
        
    # å»ºç«‹ worker pool. {id: Process}
    worker_pool = {}

    print(f"\nğŸš€ å•Ÿå‹• {args.concurrent} å€‹è‡ªè¨‚ worker...")
    start_time = time.time()
    
    # å•Ÿå‹•æ–° worker çš„è¼”åŠ©å‡½æ•¸
    def start_new_worker(worker_id):
        print(f"ğŸŒ± [Main-NEW_WORKER] æ­£åœ¨å•Ÿå‹•æ–°çš„ Worker {worker_id}...")
        p = Process(
            target=worker_process_loop, 
            args=(worker_id, task_queue, result_queue, args.max_mem_mb)
        )
        p.start()
        worker_pool[worker_id] = p

    for i in range(args.concurrent):
        start_new_worker(i)

    # --- Main è™•ç†è¿´åœˆé–‹å§‹---
    
    successful_sites = 0
    failed_sites = 0
    processed_count = 0
    crawl_success = True

    try:
        while processed_count < num_total_tasks:
            
            try:
                result = result_queue.get(timeout=600.0) 
                
                # è™•ç† RESTART
                if isinstance(result, tuple) and result[0] == "RESTART":
                    worker_id_to_restart = result[1]
                    
                    print(f"\nğŸ”¥ [Main-RESTART_PROCESS_1] æ”¶åˆ° Worker {worker_id_to_restart} çš„é‡å•Ÿè«‹æ±‚")
                    
                    # ç¢ºä¿èˆŠçš„ process è¢«æ¸…ç†
                    if worker_id_to_restart in worker_pool:
                        old_worker = worker_pool.pop(worker_id_to_restart)
                        if old_worker.is_alive():
                            print(f"â³ [[Main-RESTART_PROCESS_2] æ­£åœ¨ join èˆŠçš„ Worker {worker_id_to_restart}...")
                            old_worker.join(timeout=10) # çµ¦ 10 ç§’
                            if old_worker.is_alive():
                                print(f"âš ï¸ [[Main-RESTART_PROCESS_3] Worker {worker_id_to_restart} join è¶…æ™‚ï¼Œå¼·åˆ¶ terminate")
                                old_worker.terminate()
                                old_worker.join() # ç¢ºä¿ terminate å®Œæˆ
                    
                    # é‡æ–°å•Ÿå‹•ä¸€å€‹åŒ ID çš„æ–° worker
                    start_new_worker(worker_id_to_restart)

                # è™•ç† FAILED
                elif isinstance(result, tuple) and result[0] == "FAILED":
                    failed_site_name = result[1]
                    print(f"ğŸ“Š [Main-FAILED] æ”¶åˆ°å¤±æ•—ä»»å‹™: {failed_site_name}")
                    failed_sites += 1
                    processed_count += 1
                    print(f"ğŸ“ˆ [é€²åº¦] {processed_count} / {num_total_tasks} (æˆåŠŸ: {successful_sites}, å¤±æ•—: {failed_sites})")

                # è™•ç†æˆåŠŸçš„ä»»å‹™
                else:
                    try:
                        crawl_date = datetime.now().strftime('%Y-%m-%d %H:%M')
                        result['crawl_date'] = crawl_date
                        
                        reporter.add_site_to_excel(result) # å¯«å…¥ Excel
                        successful_sites += 1
                    except Exception as e:
                        print(f"âŒ å¯«å…¥ Excel å¤±æ•—: {e}")
                        failed_sites += 1
                    
                    processed_count += 1
                    print(f"ğŸ“ˆ [é€²åº¦] {processed_count} / {num_total_tasks} (æˆåŠŸ: {successful_sites}, å¤±æ•—: {failed_sites})")
            
            except Empty:
                # è‹¥ 10 åˆ†é˜éƒ½æ²’æœ‰ä»»ä½• worker å›å‚³çµæœæª¢æŸ¥
                print("â° [Main-CHECK] ç­‰å¾…çµæœè¶…é 10 åˆ†é˜ï¼Œæª¢æŸ¥ worker ç‹€æ…‹")
                all_dead = True
                for i, p in worker_pool.items():
                    if p.is_alive():
                        print(f"  -> Worker {i} (PID {p.pid}) ä»åœ¨åŸ·è¡Œä¸­")
                        all_dead = False
                
                if all_dead:
                    print("âŒ [Main-CHECK] æ‰€æœ‰ worker éƒ½å·²æ­»äº¡ï¼Œä½†ä»»å‹™æœªå®Œæˆï¼å¼·åˆ¶é€€å‡º...")
                    crawl_success = False
                    break # è·³å‡º while è¿´åœˆ
                else:
                    print("... ä»æœ‰ worker å­˜æ´»ï¼Œç¹¼çºŒç­‰å¾…...")

    except Exception as e:
        print(f"\nğŸ’¥ ä¸»è¿´åœˆç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        crawl_success = False
        print("ğŸš¨ [Main-TERMINATE] æ­£åœ¨çµ‚æ­¢æ‰€æœ‰ worker...")
        for p in worker_pool.values():
            if p.is_alive():
                p.terminate()
                p.join()

    finally:
        total_duration = time.time() - start_time
        total_duration_formatted = f"{int(total_duration // 60)}åˆ†{int(total_duration % 60)}ç§’"
        
        print(f"\n{'='*50}")
        print(f"ğŸ‰ ä¸¦è¡Œè™•ç†å®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸè™•ç†: {successful_sites} å€‹ç¶²ç«™")
        print(f"âŒ å¤±æ•—: {failed_sites} å€‹ç¶²ç«™") 
        print(f"â±ï¸ ç¸½è€—æ™‚: {total_duration_formatted}")
        
        reporter.finalize_excel_report()
        print(f"ğŸ“„ å ±å‘Šå·²å„²å­˜åˆ°: {output_path}\n")
        
        if crawl_success and processed_count == num_total_tasks:
            print("ğŸ‰ ä»»å‹™å…¨éƒ¨å®Œæˆï¼Œæº–å‚™æ‰“åŒ…ä¸¦ç™¼é€å ±å‘Š...")
            pack_and_send_email(output_path)
            print("æº–å‚™é—œæ©Ÿ...")
            auto_shutdown_vm()
        elif not crawl_success:
            print("ğŸ”§ ç”±æ–¼åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼ŒVM å°‡ä¿æŒé–‹å•Ÿç‹€æ…‹ä»¥ä¾¿é™¤éŒ¯")
        else:
            print(f"âš ï¸ ä»»å‹™æœªå…¨éƒ¨å®Œæˆ ({processed_count}/{num_total_tasks})ï¼ŒVM å°‡ä¿æŒé–‹å•Ÿç‹€æ…‹")


if __name__ == "__main__":
    # ç¢ºä¿ multiprocessing åœ¨ macOS/Windows ä¸Šæ­£å¸¸é‹ä½œ
    multiprocessing.freeze_support() 
    main()